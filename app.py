import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def is_repetitive_text(text):
    """
    Ki·ªÉm tra xem c√¢u c√≥ ph·∫£i l√† vƒÉn b·∫£n l·∫∑p l·∫°i kh√¥ng.
    V√≠ d·ª•: "H√° h√° h√°", "hu hu hu", "a a a"
    """
    clean_text = re.sub(r'[.!?,;:"\']', '', text).lower().strip()
    words = clean_text.split()
    
    if len(words) < 2:
        return False
    
    # Ki·ªÉm tra t·∫•t c·∫£ t·ª´ gi·ªëng nhau
    unique_words = set(words)
    if len(unique_words) == 1:
        return True
    
    # Lo·∫°i b·ªè d·∫•u thanh ƒë·ªÉ so s√°nh
    normalized_words = [''.join(c for c in word if c.isalpha()) for word in words]
    unique_normalized = set(normalized_words)
    
    # N·∫øu sau khi normalize ch·ªâ c√≤n 1-2 t·ª´ duy nh·∫•t
    if len(unique_normalized) <= 2 and len(words) >= 3:
        return True
    
    return False

def normalize_sentence_ending(sentence):
    """
    Chu·∫©n h√≥a k√Ω t·ª± k·∫øt th√∫c c√¢u:
    - N·∫øu kh√¥ng c√≥ d·∫•u ch·∫•m c√¢u ‚Üí th√™m d·∫•u ch·∫•m
    - N·∫øu c√≥ d·∫•u ch·∫•m + k√Ω t·ª± ƒë·∫∑c bi·ªát ‚Üí x√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát
    """
    sentence = sentence.strip()
    if not sentence:
        return "."
    
    valid_punctuation = '.!?'
    last_char = sentence[-1]
    
    # N·∫øu ƒë√£ c√≥ d·∫•u c√¢u h·ª£p l·ªá
    if last_char in valid_punctuation:
        return sentence
    
    # Ki·ªÉm tra c√≥ d·∫•u c√¢u ·ªü g·∫ßn cu·ªëi kh√¥ng
    for i in range(len(sentence) - 1, max(0, len(sentence) - 5), -1):
        if sentence[i] in valid_punctuation:
            # C·∫Øt b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát ph√≠a sau
            return sentence[:i+1]
    
    # Kh√¥ng c√≥ d·∫•u c√¢u ‚Üí th√™m d·∫•u ch·∫•m
    return sentence + "."

def smart_merge_sentences(text):
    """
    X·ª≠ l√Ω vƒÉn b·∫£n th√¥ng minh:
    - Merge c√¢u < 3 t·ª´ v·ªõi c√¢u ti·∫øp theo
    - Merge c√¢u l·∫∑p l·∫°i (h√° h√° h√°, a a, etc.)
    - Gi·ªØ nguy√™n c·∫•u tr√∫c vƒÉn b·∫£n g·ªëc
    - KH√îNG th√™m b·∫•t k·ª≥ t·ª´ n√†o v√†o vƒÉn b·∫£n
    
    Returns:
        str: VƒÉn b·∫£n ƒë√£ merge, ch·ªâ c√≥ d·∫•u ch·∫•m ƒë∆°n
    """
    print("\nüìù Smart sentence merging...")
    
    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p
    clean_text = text.replace('"', '').replace('"', '').replace('"', '').strip()
    
    # T√°ch th√†nh c√°c c√¢u d·ª±a tr√™n d·∫•u c√¢u
    sentences = re.split(r'([.!?]+)', clean_text)
    
    processed_sentences = []
    accumulator = []  # T√≠ch l≈©y c√¢u ng·∫Øn/l·∫∑p
    
    for i in range(0, len(sentences), 2):
        if i >= len(sentences):
            break
            
        sentence_text = sentences[i].strip()
        punctuation = sentences[i + 1] if i + 1 < len(sentences) else '.'
        
        if not sentence_text:
            continue
        
        word_count = len(sentence_text.split())
        is_repetitive = is_repetitive_text(sentence_text)
        
        print(f"   [{word_count}w, rep={is_repetitive}] '{sentence_text[:60]}...'")
        
        # Ki·ªÉm tra c√≥ c·∫ßn merge kh√¥ng
        should_merge = (word_count < 3) or is_repetitive
        
        if should_merge:
            accumulator.append(sentence_text)
            print(f"      ‚Üí Accumulating for merge")
        else:
            # C√¢u ƒë·ªß d√†i v√† kh√¥ng l·∫∑p
            if accumulator:
                # Merge v·ªõi c√°c c√¢u ƒë√£ t√≠ch l≈©y
                merged = ". ".join(accumulator + [sentence_text])
                merged = normalize_sentence_ending(merged)
                processed_sentences.append(merged)
                print(f"      ‚Üí Merged: '{merged[:60]}...'")
                accumulator = []
            else:
                # C√¢u ƒë·ªôc l·∫≠p
                normalized = normalize_sentence_ending(sentence_text + punctuation)
                processed_sentences.append(normalized)
                print(f"      ‚Üí Keep as is")
    
    # X·ª≠ l√Ω c√¢u c√≤n s√≥t
    if accumulator:
        if processed_sentences:
            # Merge v√†o c√¢u tr∆∞·ªõc
            last = processed_sentences[-1].rstrip('.!?')
            merged = last + ". " + ". ".join(accumulator)
            processed_sentences[-1] = normalize_sentence_ending(merged)
            print(f"   üîó Merged remaining to last sentence")
        else:
            # Ch·ªâ c√≥ c√¢u ng·∫Øn
            merged = ". ".join(accumulator)
            processed_sentences.append(normalize_sentence_ending(merged))
            print(f"   ‚ö†Ô∏è Only short sentences")
    
    # Gh√©p t·∫•t c·∫£ c√¢u l·∫°i b·∫±ng kho·∫£ng tr·∫Øng ƒë∆°n
    final_text = " ".join(processed_sentences)
    
    print(f"\n‚úÖ Merging done!")
    print(f"   Original: {len(text)} chars")
    print(f"   Processed: {len(final_text)} chars")
    print(f"   Sentences: {len(sentences)//2} ‚Üí {len(processed_sentences)}")
    
    return final_text

def post_process(text):
    """L√†m s·∫°ch vƒÉn b·∫£n - CH·ªà x·ª≠ l√Ω l·ªói format, KH√îNG x√≥a d·∫•u ch·∫•m."""
    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p
    text = text.replace('"', '').replace('"', '').replace('"', '')
    
    # Lo·∫°i b·ªè d·∫•u ph·∫©y/ch·∫•m tr√πng l·∫∑p
    text = re.sub(r',+', ',', text)
    text = re.sub(r'\.{3,}', '.', text)  # CH·ªà x√≥a 3 d·∫•u ch·∫•m tr·ªü l√™n (...)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = " ".join(text.split())
    
    return text

def safe_normalize(text):
    """Normalize vƒÉn b·∫£n an to√†n v·ªõi TTSnorm."""
    try:
        normalized = TTSnorm(text)
        if len(normalized.strip()) < 2:
            return text.lower()
        return normalized.lower()
    except Exception as e:
        print(f"   ‚ö†Ô∏è TTSnorm error: {e}, using original")
        return text.lower()

# Load models
print("üîÑ Loading models...")
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)
print("‚úÖ Models loaded!")

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              use_smart_merge: bool = True, request: gr.Request = None):
    """
    TTS inference - ƒê·ªçc to√†n b·ªô text m·ªôt l·∫ßn duy nh·∫•t.
    """
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    if len(gen_text.split()) > 1000:
        raise gr.Error("Please enter text content with less than 1000 words.")
    
    try:
        # B∆∞·ªõc 1: Smart merge (n·∫øu ƒë∆∞·ª£c b·∫≠t)
        if use_smart_merge:
            print("\n" + "="*60)
            print("üß† SMART MERGE ENABLED")
            print("="*60)
            processed_text = smart_merge_sentences(gen_text)
        else:
            print("\n" + "="*60)
            print("üìù SMART MERGE DISABLED - Using original text")
            print("="*60)
            processed_text = gen_text
        
        # B∆∞·ªõc 2: Post process (l√†m s·∫°ch)
        print("\nüßπ Post processing...")
        cleaned_text = post_process(processed_text)
        print(f"   After cleaning: '{cleaned_text[:100]}...'")
        
        # B∆∞·ªõc 3: Normalize v·ªõi TTSnorm
        print("\nüîÑ Normalizing with TTSnorm...")
        normalized_text = safe_normalize(cleaned_text)
        print(f"   After TTSnorm: '{normalized_text[:100]}...'")
        
        # B∆∞·ªõc 4: Ki·ªÉm tra text cu·ªëi
        final_word_count = len(normalized_text.split())
        print(f"\nüìä Final text stats:")
        print(f"   Words: {final_word_count}")
        print(f"   Chars: {len(normalized_text)}")
        print(f"   Preview: '{normalized_text[:150]}...'")
        
        # B∆∞·ªõc 5: Preprocess reference audio
        print("\nüé§ Processing reference audio...")
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        print(f"   Ref text: '{ref_text[:100]}...'")
        
        # B∆∞·ªõc 6: Generate audio (M·ªòT L·∫¶N DUY NH·∫§T)
        print("\nüéµ Generating audio (SINGLE PASS)...")
        print("="*60)
        
        final_wave, final_sample_rate, spectrogram = infer_process(
            ref_audio, 
            ref_text.lower(), 
            normalized_text,  # ƒê√ÇY L√Ä TEXT HO√ÄN CH·ªàNH, ƒê·ªåC M·ªòT L·∫¶N
            model, 
            vocoder, 
            speed=speed
        )
        
        duration = len(final_wave) / final_sample_rate
        print("="*60)
        print(f"‚úÖ Audio generated successfully!")
        print(f"   Duration: {duration:.2f}s")
        print(f"   Sample rate: {final_sample_rate}Hz")
        print(f"   Array shape: {final_wave.shape}")
        print("="*60)
        
        # B∆∞·ªõc 7: Save spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            save_spectrogram(spectrogram, spectrogram_path)

        return (final_sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        print("\n" + "="*60)
        print("‚ùå ERROR OCCURRED:")
        print("="*60)
        traceback.print_exc()
        print("="*60)
        raise gr.Error(f"Error generating voice: {str(e)}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS Vietnamese - Simple Single Pass
    ### ‚ú® Clean & Reliable Text-to-Speech
    
    **Key Features:**
    - üéØ Reads entire text in ONE pass (no chunking errors)
    - üß† Smart merge: Combines short/repetitive sentences
    - üö´ No artificial pause markers (model creates natural pauses)
    - ‚úÖ Works exactly like original code but smarter
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text to Generate", 
            placeholder="""Enter your text here...

The system will automatically:
- Merge sentences < 3 words with next sentence
- Merge repetitive text like "H√° h√° h√°", "A a a"
- Keep everything else exactly as you typed

Example:
"H√° h√° h√°!"
"A!"
"C·∫≠u v·ªÅ r·ªìi sao?"

‚Üí Will become: "H√° h√° h√°. A. C·∫≠u v·ªÅ r·ªìi sao?"
""", 
            lines=12
        )
    
    with gr.Row():
        speed = gr.Slider(
            minimum=0.3, 
            maximum=2.0, 
            value=1.0, 
            step=0.1, 
            label="‚ö° Speed"
        )
        use_smart_merge = gr.Checkbox(
            value=True,
            label="üß† Enable Smart Merge",
            info="Merge short (<3 words) and repetitive sentences"
        )
    
    btn_synthesize = gr.Button("üî• Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üìñ How It Works:
    
    | Step | Process |
    |------|---------|
    | 1Ô∏è‚É£ | **Smart Merge** (optional): Merges short/repetitive sentences |
    | 2Ô∏è‚É£ | **Post Processing**: Cleans up punctuation |
    | 3Ô∏è‚É£ | **TTSnorm**: Converts numbers/special chars to Vietnamese |
    | 4Ô∏è‚É£ | **Single-Pass TTS**: Reads entire text at once ‚úÖ |
    
    ### üéØ What Gets Merged:
    
    - ‚úÖ Sentences with **< 3 words** (e.g., "A!", "D·∫°?")
    - ‚úÖ **Repetitive text** (e.g., "H√° h√° h√°", "Hu hu hu")
    - ‚ùå Everything else stays **exactly as typed**
    
    **Merging uses only periods** - no extra words added!
    
    ### ‚öôÔ∏è Technical Details:
    
    - Model: F5-TTS trained on ~1000 hours Vietnamese data
    - Processing: Single GPU pass (no chunking)
    - Pause control: Natural pauses at periods (model-based)
    - Max length: 1000 words per generation
    
    ### üìå Why Single Pass?
    
    ‚úÖ **More stable** - no audio concatenation issues  
    ‚úÖ **Better flow** - natural prosody across entire text  
    ‚úÖ **Fewer errors** - eliminates chunking problems  
    ‚úÖ **Faster** - one model inference instead of many  
    
    **Note:** Pause duration is controlled by the model based on punctuation. 
    You cannot manually adjust silence length in single-pass mode.
    """)
    
    with gr.Accordion("‚ùó Limitations & Tips", open=False):
        gr.Markdown("""
        **Limitations:**
        1. Numbers/dates may not pronounce correctly (needs better normalization)
        2. Reference audio quality affects output quality
        3. Very long texts (>1000 words) may fail or produce poor quality
        4. Foreign words pronounced phonetically in Vietnamese
        
        **Tips for Best Results:**
        - Use clear reference audio (15-30s, no background noise)
        - Keep text under 1000 words
        - Use proper punctuation for natural pauses
        - Enable smart merge for dialogue-heavy text
        - Disable smart merge for poetry or carefully crafted text
        """)

    # Connect button
    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, use_smart_merge], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)
