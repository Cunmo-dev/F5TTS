import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re
import numpy as np

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def split_text_into_sentences(text, pause_paragraph_duration=0.8, pause_dialogue_duration=0.4):
    """
    T√°ch vƒÉn b·∫£n theo chu·∫©n repo g·ªëc:
    - Merge c√¢u < 4 t·ª´ b·∫±ng d·∫•u PH·∫®Y (kh√¥ng ph·∫£i ch·∫•m)
    - Merge v·ªõi c√¢u tr∆∞·ªõc n·∫øu c√≥, n·∫øu kh√¥ng th√¨ c√¢u sau
    """
    chunks = []
    
    # T√°ch theo d√≤ng tr·ªëng
    paragraphs = text.split('\n\n')
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # Ki·ªÉm tra h·ªôi tho·∫°i
        lines = para.split('\n')
        combined_text = ' '.join(line.strip() for line in lines if line.strip())
        
        open_quotes = combined_text.count('"') + combined_text.count('"')
        close_quotes = combined_text.count('"') + combined_text.count('"')
        
        is_dialogue = (open_quotes > 0 and open_quotes == close_quotes)
        pause_duration = pause_dialogue_duration if is_dialogue else pause_paragraph_duration
        
        # Lo·∫°i b·ªè ngo·∫∑c k√©p
        clean_text = combined_text.replace('"', '').replace('"', '').replace('"', '').strip()
        
        # T√°ch c√¢u
        sentences = re.split(r'([.!?]+)', clean_text)
        
        current_sentence = ""
        for i, part in enumerate(sentences):
            if i % 2 == 0:
                current_sentence += part
            else:
                current_sentence += part
                sentence_text = current_sentence.strip()
                if sentence_text:
                    chunks.append((sentence_text, pause_duration))
                    current_sentence = ""
        
        if current_sentence.strip():
            chunks.append((current_sentence.strip(), pause_duration))
    
    # ===== LOGIC MERGE THEO REPO G·ªêC =====
    # Merge c√¢u < 4 t·ª´ b·∫±ng D·∫§U PH·∫®Y
    i = 0
    while i < len(chunks):
        sentence, pause = chunks[i]
        word_count = len(sentence.split())
        
        if word_count < 4:
            if i == 0 and len(chunks) > 1:
                # C√¢u ƒë·∫ßu ti√™n: merge v·ªõi c√¢u SAU b·∫±ng ph·∫©y
                next_sentence, next_pause = chunks[i + 1]
                merged = sentence + ', ' + next_sentence
                chunks[i] = (merged, next_pause)
                del chunks[i + 1]
                print(f"   üîó Merged first short sentence: '{sentence}' + '{next_sentence[:30]}...'")
            elif i > 0:
                # C√¢u gi·ªØa/cu·ªëi: merge v·ªõi c√¢u TR∆Ø·ªöC b·∫±ng ph·∫©y
                prev_sentence, prev_pause = chunks[i - 1]
                merged = prev_sentence + ', ' + sentence
                chunks[i - 1] = (merged, prev_pause)
                del chunks[i]
                i -= 1  # L√πi l·∫°i ƒë·ªÉ ki·ªÉm tra c√¢u ti·∫øp theo
                print(f"   üîó Merged short sentence with previous: '{sentence}'")
            else:
                # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: ch·ªâ c√≥ 1 c√¢u ng·∫Øn
                # L·∫∑p l·∫°i t·ª´ cu·ªëi ƒë·ªÉ ƒë·ªß 4 t·ª´
                words = sentence.split()
                while len(words) < 4:
                    # L·∫•y t·ª´ cu·ªëi (b·ªè d·∫•u c√¢u n·∫øu c√≥)
                    last_word = re.sub(r'[.,!?;:\-‚Ä¶]+$', '', words[-1])
                    words.append(last_word)
                padded = ' '.join(words)
                chunks[i] = (padded, pause)
                print(f"   ‚ö†Ô∏è Padded single short sentence: '{sentence}' ‚Üí '{padded}'")
        
        i += 1
    
    return chunks

def create_silence(duration_seconds, sample_rate=24000):
    """T·∫°o ƒëo·∫°n im l·∫∑ng."""
    num_samples = int(duration_seconds * sample_rate)
    return np.zeros(num_samples, dtype=np.float32)

def post_process(text):
    """L√†m s·∫°ch vƒÉn b·∫£n - lo·∫°i b·ªè t·∫•t c·∫£ d·∫•u c√¢u d∆∞ th·ª´a."""
    text = " " + text + " "
    
    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p
    text = text.replace('"', "")
    text = text.replace('"', "")
    text = text.replace('"', "")
    
    # Lo·∫°i b·ªè d·∫•u ch·∫•m/ph·∫©y/ch·∫•m than/ch·∫•m h·ªèi d∆∞ th·ª´a
    text = re.sub(r'\.{2,}', '.', text)  # Nhi·ªÅu d·∫•u ch·∫•m ‚Üí 1 d·∫•u ch·∫•m
    text = re.sub(r',+', ',', text)      # Nhi·ªÅu d·∫•u ph·∫©y ‚Üí 1 d·∫•u ph·∫©y
    text = re.sub(r'!+', '!', text)      # Nhi·ªÅu d·∫•u ch·∫•m than ‚Üí 1
    text = re.sub(r'\?+', '?', text)     # Nhi·ªÅu d·∫•u h·ªèi ‚Üí 1
    
    # Lo·∫°i b·ªè d·∫•u c√¢u ·ªü cu·ªëi (TTS kh√¥ng c·∫ßn)
    text = re.sub(r'[.,!?;:\-‚Ä¶]+

def safe_normalize(text):
    """Normalize vƒÉn b·∫£n an to√†n - B·ªé QUA c√°c c√¢u l·∫∑p t·ª´ ƒë∆°n gi·∫£n."""
    # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u l·∫∑p t·ª´ ƒë∆°n gi·∫£n kh√¥ng (h√° h√°, h√™ h√™, √† √†...)
    words = text.lower().strip().split()
    unique_words = set(re.sub(r'[.,!?;:\-‚Ä¶]+', '', w) for w in words)
    
    # N·∫øu ch·ªâ c√≥ 1-2 t·ª´ duy nh·∫•t ƒë∆∞·ª£c l·∫∑p l·∫°i ‚Üí KH√îNG normalize
    if len(unique_words) <= 2 and len(words) <= 5:
        cleaned = re.sub(r'[.,!?;:\-‚Ä¶]+', '', text.lower().strip())
        print(f"   üé≠ Detected repetitive pattern, skipped normalize: '{cleaned}'")
        return cleaned
    
    # C√°c c√¢u b√¨nh th∆∞·ªùng ‚Üí normalize
    try:
        normalized = TTSnorm(text)
        if len(normalized.strip()) < 2:
            return text.lower()
        return normalized.lower()
    except Exception as e:
        print(f"   ‚ö†Ô∏è TTSnorm error: {e}, using original text")
        return text.lower()

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_level: str = "Medium", request: gr.Request = None):
    """TTS inference v·ªõi pause th·ª±c s·ª±."""
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        pause_configs = {
            "Short": (0.2, 0.1),
            "Medium": (0.4, 0.2),
            "Long": (0.6, 0.3)
        }
        
        pause_paragraph, pause_dialogue = pause_configs.get(pause_level, (0.4, 0.2))
        print(f"\nüéõÔ∏è Pause config: Paragraph={pause_paragraph}s, Dialogue={pause_dialogue}s")
        
        # T√°ch vƒÉn b·∫£n
        chunks = split_text_into_sentences(gen_text, pause_paragraph, pause_dialogue)
        
        print(f"\nüìù Total chunks after merge: {len(chunks)}")
        for idx, (sent, pause) in enumerate(chunks, 1):
            word_count = len(sent.split())
            print(f"   {idx}. [{word_count} words, {pause}s] {sent[:100]}...")
        
        if not chunks:
            raise gr.Error("No valid sentences found in text.")
        
        # Preprocess reference audio
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # Generate audio
        audio_segments = []
        sample_rate = 24000
        
        for i, (sentence, pause_duration) in enumerate(chunks):
            print(f"\nüîÑ [{i+1}/{len(chunks)}] Processing...")
            print(f"   üìÑ Original: {sentence[:100]}")
            
            # Normalize
            normalized_text = post_process(safe_normalize(sentence))
            normalized_text = ' '.join(normalized_text.split())  # Clean whitespace
            
            word_count = len(normalized_text.strip().split())
            print(f"   üìù Normalized ({word_count} words): {normalized_text[:100]}")
            
            # QUAN TR·ªåNG: N·∫øu < 3 t·ª´ ‚Üí pad th√™m
            if word_count < 3:
                # L·∫∑p l·∫°i to√†n b·ªô text
                original_words = normalized_text.split()
                while len(original_words) < 3:
                    original_words.extend(normalized_text.split())
                normalized_text = ' '.join(original_words[:5])  # T·ªëi ƒëa 5 t·ª´
                print(f"   ‚ûï Padded to {len(normalized_text.split())} words: {normalized_text}")
            
            # Ki·ªÉm tra l·∫°i sau khi pad
            final_word_count = len(normalized_text.strip().split())
            if final_word_count < 2:
                print(f"   ‚è≠Ô∏è Still too short after padding, skipping")
                continue
            
            # Retry v·ªõi backoff
            max_retries = 3
            success = False
            
            for retry in range(max_retries + 1):
                try:
                    if retry > 0:
                        print(f"   üîÅ Retry {retry}/{max_retries}...")
                        # Th·ª≠ l√†m s·∫°ch h∆°n n·ªØa
                        normalized_text = re.sub(r'[^\w\s]', '', normalized_text)
                        normalized_text = ' '.join(normalized_text.split())
                        print(f"   üßπ Extra cleaned: {normalized_text}")
                    
                    print(f"   üé§ Calling TTS with: ref_text='{ref_text[:30]}', gen_text='{normalized_text[:50]}'")
                    
                    wave, sr, _ = infer_process(
                        ref_audio, 
                        ref_text.lower(), 
                        normalized_text, 
                        model, 
                        vocoder, 
                        speed=speed
                    )
                    
                    sample_rate = sr
                    audio_segments.append(wave)
                    print(f"   ‚úÖ Generated {len(wave)/sr:.2f}s audio")
                    success = True
                    
                    # Add silence between chunks
                    if i < len(chunks) - 1:
                        silence = create_silence(pause_duration, sample_rate)
                        audio_segments.append(silence)
                        print(f"   ‚è∏Ô∏è  Added {pause_duration}s silence")
                    
                    break
                    
                except Exception as e:
                    error_msg = str(e)
                    print(f"   ‚ùå Attempt {retry + 1} failed: {error_msg[:100]}")
                    
                    if retry == max_retries:
                        print(f"   ‚ö†Ô∏è Max retries reached, skipping this chunk")
                        print(f"   üìä Debug info: text_length={len(normalized_text)}, word_count={len(normalized_text.split())}")
                    else:
                        import time
                        time.sleep(0.5)
        
        if not audio_segments:
            raise gr.Error("No audio generated. Please check your text.")
        
        # Concat all audio
        final_wave = np.concatenate(audio_segments)
        print(f"\n‚úÖ Final audio: {len(final_wave)/sample_rate:.2f}s from {len(chunks)} chunks")
        
        # Create spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spec:
            spectrogram_path = tmp_spec.name
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            plt.specgram(final_wave, Fs=sample_rate, cmap='viridis')
            plt.xlabel('Time (s)')
            plt.ylabel('Frequency (Hz)')
            plt.title('Audio Spectrogram')
            plt.colorbar(format='%+2.0f dB')
            plt.tight_layout()
            plt.savefig(spectrogram_path)
            plt.close()

        return (sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error: {str(e)}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech (Fixed - Repo Standard)
    
    ‚ú® **Following original repo logic**: Short sentences (< 4 words) merged with COMMA
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text to Generate", 
            placeholder="""Example:
Ch·ªõp m·∫Øt m·ªôt c√°i b·ªóng d∆∞ng kh√¥ng c√≤n nh√¨n th·∫•y b√© T∆∞ ƒë√¢u n·ªØa. Trong b√≥ng ƒë√™m d√†y ƒë·∫∑c ch·ªâ nghe th·∫•y ti·∫øng c∆∞·ªùi qu·ª∑ d·ªã c·ªßa y.

"H√° h√° h√°..."

Minh Huy cƒÉng m·∫Øt nh√¨n ra xung quanh. M·ªì h√¥i tr√™n tr√°n r∆°i xu·ªëng mi m·∫Øt h·∫Øn m·ªôt m·∫£ng cay x√®.

"A!!!!!!!"
""", 
            lines=10
        )
    
    with gr.Row():
        speed = gr.Slider(0.3, 2.0, 1.0, 0.1, label="‚ö° Speed")
        pause_level = gr.Radio(
            ["Short", "Medium", "Long"],
            value="Medium",
            label="‚è∏Ô∏è Pause Duration"
        )
    
    btn = gr.Button("üî• Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spec = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üí° Logic theo repo g·ªëc:
    
    - **C√¢u < 4 t·ª´**: Merge v·ªõi c√¢u tr∆∞·ªõc/sau b·∫±ng **d·∫•u ph·∫©y** (`,`)
    - **C√¢u ƒë·∫ßu ti√™n ng·∫Øn**: Merge v·ªõi c√¢u sau
    - **C√¢u gi·ªØa/cu·ªëi ng·∫Øn**: Merge v·ªõi c√¢u tr∆∞·ªõc
    - **Ch·ªâ 1 c√¢u ng·∫Øn**: L·∫∑p l·∫°i t·ª´ cu·ªëi ƒë·ªÉ ƒë·ªß 4 t·ª´
    
    ### üìñ V√≠ d·ª• x·ª≠ l√Ω:
    ```
    Input:
    "Ti·∫øng c∆∞·ªùi c·ªßa y.
    H√° h√° h√°...
    Minh Huy cƒÉng m·∫Øt."
    
    ‚Üí "H√° h√° h√°..." ch·ªâ c√≥ 1 t·ª´ (< 4)
    ‚Üí Merge v·ªõi c√¢u tr∆∞·ªõc: "Ti·∫øng c∆∞·ªùi c·ªßa y, h√° h√° h√°..."
    ‚Üí C√¢u n√†y gi·ªù c√≥ 7 t·ª´ ‚Üí OK ‚úÖ
    ```
    """)

    btn.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, pause_level], 
        outputs=[output_audio, output_spec]
    )

demo.queue().launch(share=True)
, '', text.strip())
    
    return " ".join(text.split())

def safe_normalize(text):
    """Normalize vƒÉn b·∫£n an to√†n."""
    try:
        normalized = TTSnorm(text)
        if len(normalized.strip()) < 2:
            return text.lower()
        return normalized.lower()
    except Exception as e:
        print(f"   ‚ö†Ô∏è TTSnorm error: {e}, using original text")
        return text.lower()

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_level: str = "Medium", request: gr.Request = None):
    """TTS inference v·ªõi pause th·ª±c s·ª±."""
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        pause_configs = {
            "Short": (0.2, 0.1),
            "Medium": (0.4, 0.2),
            "Long": (0.6, 0.3)
        }
        
        pause_paragraph, pause_dialogue = pause_configs.get(pause_level, (0.4, 0.2))
        print(f"\nüéõÔ∏è Pause config: Paragraph={pause_paragraph}s, Dialogue={pause_dialogue}s")
        
        # T√°ch vƒÉn b·∫£n
        chunks = split_text_into_sentences(gen_text, pause_paragraph, pause_dialogue)
        
        print(f"\nüìù Total chunks after merge: {len(chunks)}")
        for idx, (sent, pause) in enumerate(chunks, 1):
            word_count = len(sent.split())
            print(f"   {idx}. [{word_count} words, {pause}s] {sent[:100]}...")
        
        if not chunks:
            raise gr.Error("No valid sentences found in text.")
        
        # Preprocess reference audio
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # Generate audio
        audio_segments = []
        sample_rate = 24000
        
        for i, (sentence, pause_duration) in enumerate(chunks):
            print(f"\nüîÑ [{i+1}/{len(chunks)}] Processing...")
            
            # Normalize
            normalized_text = post_process(safe_normalize(sentence))
            normalized_text = ' '.join(normalized_text.split())  # Clean whitespace
            
            word_count = len(normalized_text.strip().split())
            print(f"   üìù Text ({word_count} words): {normalized_text[:100]}...")
            
            # Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu
            if word_count < 3:
                print(f"   ‚ö†Ô∏è Text too short after normalization, skipping")
                continue
            
            # Retry v·ªõi backoff
            max_retries = 3
            success = False
            
            for retry in range(max_retries + 1):
                try:
                    if retry > 0:
                        print(f"   üîÅ Retry {retry}/{max_retries}...")
                    
                    wave, sr, _ = infer_process(
                        ref_audio, 
                        ref_text.lower(), 
                        normalized_text, 
                        model, 
                        vocoder, 
                        speed=speed
                    )
                    
                    sample_rate = sr
                    audio_segments.append(wave)
                    print(f"   ‚úÖ Generated {len(wave)/sr:.2f}s audio")
                    success = True
                    
                    # Add silence between chunks
                    if i < len(chunks) - 1:
                        silence = create_silence(pause_duration, sample_rate)
                        audio_segments.append(silence)
                        print(f"   ‚è∏Ô∏è  Added {pause_duration}s silence")
                    
                    break
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Attempt {retry + 1} failed: {str(e)[:80]}")
                    if retry == max_retries:
                        print(f"   ‚ùå Max retries reached, skipping chunk")
                    else:
                        import time
                        time.sleep(0.5)
        
        if not audio_segments:
            raise gr.Error("No audio generated. Please check your text.")
        
        # Concat all audio
        final_wave = np.concatenate(audio_segments)
        print(f"\n‚úÖ Final audio: {len(final_wave)/sample_rate:.2f}s from {len(chunks)} chunks")
        
        # Create spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spec:
            spectrogram_path = tmp_spec.name
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            plt.specgram(final_wave, Fs=sample_rate, cmap='viridis')
            plt.xlabel('Time (s)')
            plt.ylabel('Frequency (Hz)')
            plt.title('Audio Spectrogram')
            plt.colorbar(format='%+2.0f dB')
            plt.tight_layout()
            plt.savefig(spectrogram_path)
            plt.close()

        return (sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error: {str(e)}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech (Fixed - Repo Standard)
    
    ‚ú® **Following original repo logic**: Short sentences (< 4 words) merged with COMMA
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text to Generate", 
            placeholder="""Example:
Ch·ªõp m·∫Øt m·ªôt c√°i b·ªóng d∆∞ng kh√¥ng c√≤n nh√¨n th·∫•y b√© T∆∞ ƒë√¢u n·ªØa. Trong b√≥ng ƒë√™m d√†y ƒë·∫∑c ch·ªâ nghe th·∫•y ti·∫øng c∆∞·ªùi qu·ª∑ d·ªã c·ªßa y.

"H√° h√° h√°..."

Minh Huy cƒÉng m·∫Øt nh√¨n ra xung quanh. M·ªì h√¥i tr√™n tr√°n r∆°i xu·ªëng mi m·∫Øt h·∫Øn m·ªôt m·∫£ng cay x√®.

"A!!!!!!!"
""", 
            lines=10
        )
    
    with gr.Row():
        speed = gr.Slider(0.3, 2.0, 1.0, 0.1, label="‚ö° Speed")
        pause_level = gr.Radio(
            ["Short", "Medium", "Long"],
            value="Medium",
            label="‚è∏Ô∏è Pause Duration"
        )
    
    btn = gr.Button("üî• Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spec = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üí° Logic theo repo g·ªëc:
    
    - **C√¢u < 4 t·ª´**: Merge v·ªõi c√¢u tr∆∞·ªõc/sau b·∫±ng **d·∫•u ph·∫©y** (`,`)
    - **C√¢u ƒë·∫ßu ti√™n ng·∫Øn**: Merge v·ªõi c√¢u sau
    - **C√¢u gi·ªØa/cu·ªëi ng·∫Øn**: Merge v·ªõi c√¢u tr∆∞·ªõc
    - **Ch·ªâ 1 c√¢u ng·∫Øn**: L·∫∑p l·∫°i t·ª´ cu·ªëi ƒë·ªÉ ƒë·ªß 4 t·ª´
    
    ### üìñ V√≠ d·ª• x·ª≠ l√Ω:
    ```
    Input:
    "Ti·∫øng c∆∞·ªùi c·ªßa y.
    H√° h√° h√°...
    Minh Huy cƒÉng m·∫Øt."
    
    ‚Üí "H√° h√° h√°..." ch·ªâ c√≥ 1 t·ª´ (< 4)
    ‚Üí Merge v·ªõi c√¢u tr∆∞·ªõc: "Ti·∫øng c∆∞·ªùi c·ªßa y, h√° h√° h√°..."
    ‚Üí C√¢u n√†y gi·ªù c√≥ 7 t·ª´ ‚Üí OK ‚úÖ
    ```
    """)

    btn.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, pause_level], 
        outputs=[output_audio, output_spec]
    )

demo.queue().launch(share=True)
