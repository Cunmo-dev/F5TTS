import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re
import numpy as np

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def post_process(text):
    """
    X·ª≠ l√Ω vƒÉn b·∫£n v·ªõi c√°c quy t·∫Øc:
    1. Thay th·∫ø t·∫•t c·∫£ d·∫•u ph·∫©y b·∫±ng d·∫•u ch·∫•m ƒë·ªÉ m·ªói c√¢u ng·∫Øn ƒë·ªÅu ƒë·ªôc l·∫≠p
    2. C√¢u trong d·∫•u ngo·∫∑c k√©p "" ƒë∆∞·ª£c coi l√† c√¢u ri√™ng bi·ªát
    3. Th√™m d·∫•u ch·∫•m tr∆∞·ªõc d·∫•u ngo·∫∑c k√©p m·ªü
    4. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát trong d·∫•u ngo·∫∑c k√©p
    5. N·∫øu k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü cu·ªëi c√¢u trong ngo·∫∑c k√©p, thay b·∫±ng d·∫•u ch·∫•m
    6. Lo·∫°i b·ªè d·∫•u ph·∫©y/ch·∫•m tr√πng l·∫∑p trong ngo·∫∑c k√©p
    7. X·ª≠ l√Ω c√¢u ngo√†i d·∫•u ngo·∫∑c k√©p: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát cu·ªëi c√¢u v√† th√™m d·∫•u ch·∫•m
    """
    
    # ƒê√°nh d·∫•u c√°c ƒëo·∫°n text trong d·∫•u ngo·∫∑c k√©p ƒë·ªÉ tr√°nh x·ª≠ l√Ω nh·∫ßm
    quoted_sections = []
    placeholder_pattern = "<<<QUOTED_{}>>>"
    
    def save_quoted_text(match):
        index = len(quoted_sections)
        quoted_sections.append(match.group(0))
        return placeholder_pattern.format(index)
    
    # T·∫°m th·ªùi thay th·∫ø c√°c ƒëo·∫°n text trong ngo·∫∑c k√©p b·∫±ng placeholder
    text = re.sub(r'"[^"]*"', save_quoted_text, text)
    
    # THAY T·∫§T C·∫¢ D·∫§U PH·∫®Y B·∫∞NG D·∫§U CH·∫§M (ngo√†i d·∫•u ngo·∫∑c k√©p)
    text = text.replace(',', '.')
    
    # X·ª≠ l√Ω text ngo√†i d·∫•u ngo·∫∑c k√©p
    # T√°ch th√†nh c√°c ph·∫ßn d·ª±a tr√™n placeholder
    parts = re.split(r'(<<<QUOTED_\d+>>>)', text)
    
    processed_parts = []
    special_chars_pattern = r'[!@#$%^&*()_+=\[\]{};:\\|<>/?~`"\']'
    
    for part in parts:
        if part.startswith('<<<QUOTED_') and part.endswith('>>>'):
            # ƒê√¢y l√† placeholder, gi·ªØ nguy√™n
            processed_parts.append(part)
        else:
            # X·ª≠ l√Ω text ngo√†i ngo·∫∑c k√©p
            if part.strip():
                # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü cu·ªëi c√¢u
                part = part.rstrip()
                if part:
                    # Lo·∫°i b·ªè t·∫•t c·∫£ k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü cu·ªëi (kh√¥ng bao g·ªìm d·∫•u ch·∫•m)
                    while part and re.search(special_chars_pattern + r'$', part):
                        part = re.sub(special_chars_pattern + r'$', '', part).rstrip()
                    
                    # Th√™m d·∫•u ch·∫•m n·∫øu ch∆∞a c√≥
                    if part and not part.endswith('.'):
                        part += '.'
            
            processed_parts.append(part)
    
    text = ''.join(processed_parts)
    
    # X·ª≠ l√Ω c√°c ƒëo·∫°n text trong d·∫•u ngo·∫∑c k√©p
    def process_quoted_text(quoted_with_marks):
        # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p ƒë·ªÉ x·ª≠ l√Ω n·ªôi dung
        quoted = quoted_with_marks.strip('"')
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë, kho·∫£ng tr·∫Øng, d·∫•u ph·∫©y v√† d·∫•u ch·∫•m)
        special_chars_pattern = r'[!@#$%^&*()_+=\[\]{};:\\|<>/?~`]'
        quoted = re.sub(special_chars_pattern, '', quoted)
        
        # Lo·∫°i b·ªè d·∫•u ph·∫©y v√† d·∫•u ch·∫•m tr√πng l·∫∑p (ch·ªâ gi·ªØ 1 d·∫•u)
        quoted = re.sub(r'\.{2,}', '.', quoted)  # Nhi·ªÅu d·∫•u ch·∫•m -> 1 d·∫•u ch·∫•m
        quoted = re.sub(r',{2,}', ',', quoted)   # Nhi·ªÅu d·∫•u ph·∫©y -> 1 d·∫•u ph·∫©y
        quoted = re.sub(r'[,\s]+\.', '.', quoted)  # D·∫•u ph·∫©y + d·∫•u ch·∫•m -> d·∫•u ch·∫•m
        quoted = re.sub(r'\.[,\s]+', '. ', quoted)  # D·∫•u ch·∫•m + d·∫•u ph·∫©y -> d·∫•u ch·∫•m
        
        # X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü cu·ªëi c√¢u trong ngo·∫∑c k√©p
        quoted = quoted.strip()
        
        # N·∫øu c√¢u kh√¥ng k·∫øt th√∫c b·∫±ng d·∫•u ch·∫•m, th√™m d·∫•u ch·∫•m
        if quoted and not quoted.endswith('.'):
            # Lo·∫°i b·ªè d·∫•u ph·∫©y cu·ªëi c√πng n·∫øu c√≥
            if quoted.endswith(','):
                quoted = quoted[:-1].strip()
            quoted += '.'
        
        # Tr·∫£ v·ªÅ v·ªõi d·∫•u ch·∫•m tr∆∞·ªõc ngo·∫∑c k√©p m·ªü
        return '. "' + quoted + '"'
    
    # Kh√¥i ph·ª•c c√°c ƒëo·∫°n text trong ngo·∫∑c k√©p v√† x·ª≠ l√Ω ch√∫ng
    for i, quoted_section in enumerate(quoted_sections):
        placeholder = placeholder_pattern.format(i)
        processed_quoted = process_quoted_text(quoted_section)
        text = text.replace(placeholder, processed_quoted)
    
    # X·ª≠ l√Ω c√°c d·∫•u ch·∫•m tr√πng l·∫∑p
    text = " " + text + " "
    text = text.replace(" . . ", " . ")
    text = " " + text + " "
    text = text.replace(" .. ", " . ")
    text = " " + text + " "
    # Lo·∫°i b·ªè pattern ". ." nhi·ªÅu l·∫ßn
    while " . . " in text:
        text = text.replace(" . . ", " . ")
    
    # Lo·∫°i b·ªè d·∫•u ch·∫•m th·ª´a ·ªü ƒë·∫ßu c√¢u (n·∫øu c√≥)
    text = re.sub(r'^\.\s+', '', text.strip())
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = " ".join(text.split())
    
    return text

def split_sentences(text):
    """
    Chia vƒÉn b·∫£n th√†nh c√°c c√¢u d·ª±a tr√™n d·∫•u ch·∫•m
    """
    # Chia theo d·∫•u ch·∫•m, lo·∫°i b·ªè c√¢u r·ªóng
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return sentences

def add_silence(audio_array, sample_rate, silence_duration):
    """
    Th√™m kho·∫£ng l·∫∑ng v√†o cu·ªëi audio
    audio_array: numpy array c·ªßa audio
    sample_rate: t·∫ßn s·ªë l·∫•y m·∫´u
    silence_duration: th·ªùi gian l·∫∑ng (gi√¢y)
    """
    silence_samples = int(sample_rate * silence_duration)
    silence = np.zeros(silence_samples, dtype=audio_array.dtype)
    return np.concatenate([audio_array, silence])

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, silence_duration: float = 0.0, request: gr.Request = None):

    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        # X·ª≠ l√Ω vƒÉn b·∫£n
        processed_text = post_process(TTSnorm(gen_text)).lower()
        
        # Chia th√†nh c√°c c√¢u
        sentences = split_sentences(processed_text)
        
        if not sentences:
            raise gr.Error("No valid sentences found after text processing.")
        
        # Ti·ªÅn x·ª≠ l√Ω audio tham chi·∫øu
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # T·ªïng h·ª£p t·ª´ng c√¢u v√† n·ªëi l·∫°i
        all_waves = []
        all_spectrograms = []
        
        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue
                
            # Sinh audio cho c√¢u hi·ªán t·∫°i
            wave, sample_rate, spectrogram = infer_process(
                ref_audio, 
                ref_text.lower(), 
                sentence, 
                model, 
                vocoder, 
                speed=speed
            )
            
            # Th√™m kho·∫£ng l·∫∑ng n·∫øu kh√¥ng ph·∫£i c√¢u cu·ªëi
            if i < len(sentences) - 1 and silence_duration > 0:
                wave = add_silence(wave, sample_rate, silence_duration)
            
            all_waves.append(wave)
            all_spectrograms.append(spectrogram)
        
        # N·ªëi t·∫•t c·∫£ c√°c audio l·∫°i
        final_wave = np.concatenate(all_waves)
        
        # L∆∞u spectrogram c·ªßa c√¢u ƒë·∫ßu ti√™n (ho·∫∑c c√≥ th·ªÉ t·ªïng h·ª£p)
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            save_spectrogram(all_spectrograms[0], spectrogram_path)

        return (sample_rate, final_wave), spectrogram_path
    except Exception as e:
        raise gr.Error(f"Error generating voice: {e}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech Synthesis.
    # The model was trained with approximately 1000 hours of data on a RTX 3090 GPU. 
    Enter text and upload a sample voice to generate natural speech.
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(label="üìù Text", placeholder="Enter the text to generate voice...", lines=3)
    
    with gr.Row():
        speed = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° Speed")
        silence_duration = gr.Slider(0.0, 2.0, value=0.3, step=0.1, label="üîá Silence Between Sentences (seconds)")
    
    btn_synthesize = gr.Button("üî• Generate Voice")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram")
    
    model_limitations = gr.Textbox(
        value="""1. This model may not perform well with numerical characters, dates, special characters, etc. => A text normalization module is needed.
2. The rhythm of some generated audios may be inconsistent or choppy => It is recommended to select clearly pronounced sample audios with minimal pauses for better synthesis quality.
3. Default, reference audio text uses the pho-whisper-medium model, which may not always accurately recognize Vietnamese, resulting in poor voice synthesis quality.
4. Inference with overly long paragraphs may produce poor results.
5. The silence slider adds pauses between sentences split by periods (.) for better audio clarity.""", 
        label="‚ùó Model Limitations",
        lines=5,
        interactive=False
    )

    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, silence_duration], 
        outputs=[output_audio, output_spectrogram]
    )

# Run Gradio with share=True to get a gradio.live link
demo.queue().launch(share=True)
