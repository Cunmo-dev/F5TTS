import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re
import numpy as np

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

try:
    from unidecode import unidecode
    from langdetect import detect, LangDetectException
    LANG_DETECT_AVAILABLE = True
except ImportError:
    LANG_DETECT_AVAILABLE = False
    print("âš ï¸ Warning: langdetect not installed. Foreign word detection disabled.")
    print("   Install with: pip install langdetect unidecode")

def is_vietnamese_char(char):
    """Kiá»ƒm tra kÃ½ tá»± cÃ³ pháº£i tiáº¿ng Viá»‡t khÃ´ng."""
    vietnamese_chars = set('aÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­eÃ©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡iÃ­Ã¬á»‰Ä©á»‹oÃ³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£uÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±yÃ½á»³á»·á»¹á»µÄ‘')
    vietnamese_chars.update('AÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬EÃ‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†IÃÃŒá»ˆÄ¨á»ŠOÃ“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢UÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°YÃá»²á»¶á»¸á»´Ä')
    return char.lower() in vietnamese_chars or not char.isalpha()

def is_vietnamese_word(word):
    """Kiá»ƒm tra tá»« cÃ³ pháº£i tiáº¿ng Viá»‡t khÃ´ng dá»±a trÃªn tá»· lá»‡ kÃ½ tá»±."""
    clean_word = ''.join(c for c in word if c.isalpha())
    if not clean_word:
        return True  # Sá»‘, dáº¥u cÃ¢u â†’ giá»¯ nguyÃªn
    
    viet_chars = sum(1 for c in clean_word if is_vietnamese_char(c))
    return viet_chars / len(clean_word) >= 0.5  # Ãt nháº¥t 50% lÃ  kÃ½ tá»± Viá»‡t

def transliterate_to_vietnamese(text, lang=None):
    """
    Chuyá»ƒn vÄƒn báº£n ngoáº¡i ngá»¯ sang phiÃªn Ã¢m tiáº¿ng Viá»‡t.
    
    PhÆ°Æ¡ng phÃ¡p:
    1. Detect ngÃ´n ngá»¯ (náº¿u chÆ°a biáº¿t)
    2. Chuyá»ƒn sang phiÃªn Ã¢m Latin (unidecode)
    3. Äiá»u chá»‰nh phÃ¡t Ã¢m cho tiáº¿ng Viá»‡t
    """
    if not text.strip():
        return text
    
    # PhÃ¡t hiá»‡n ngÃ´n ngá»¯
    if lang is None and LANG_DETECT_AVAILABLE:
        try:
            lang = detect(text)
        except LangDetectException:
            lang = 'unknown'
    
    # Xá»­ lÃ½ theo ngÃ´n ngá»¯
    result = text.lower()
    
    # 1. Chuyá»ƒn sang phiÃªn Ã¢m Latin cÆ¡ báº£n
    if LANG_DETECT_AVAILABLE:
        result = unidecode(result)
    
    # 2. Äiá»u chá»‰nh phÃ¡t Ã¢m cho tiáº¿ng Viá»‡t
    replacements = {
        # Tiáº¿ng Anh
        'th': 'Ä‘',      # the â†’ Ä‘Æ¡
        'ch': 'ch',     # change â†’ chanh
        'sh': 's',      # shop â†’ sá»‘p
        'ph': 'f',      # phone â†’ fÃ´n
        
        # Phá»¥ Ã¢m cuá»‘i
        'ck': 'c',      # back â†’ báº¯c
        'ng': 'ng',     # king â†’ kÃ­ng
        'tion': 'sÆ¡n',  # action â†’ áº¯c sÆ¡n
        
        # NguyÃªn Ã¢m
        'oo': 'u',      # book â†’ bÃºc
        'ee': 'i',      # see â†’ xi
        'ea': 'i',      # tea â†’ ti
        'ou': 'ao',     # house â†’ hao
        'ow': 'ao',     # now â†’ nao
    }
    
    # Ãp dá»¥ng quy táº¯c chuyá»ƒn Ä‘á»•i
    for pattern, replacement in replacements.items():
        result = result.replace(pattern, replacement)
    
    # 3. XÃ³a cÃ¡c phá»¥ Ã¢m cuá»‘i khÃ³ phÃ¡t Ã¢m
    # k, t, p á»Ÿ cuá»‘i tá»« â†’ thÃªm thanh ngáº¯n
    result = re.sub(r'([ktp])(\s|$)', r'\1Æ¡\2', result)
    
    print(f"   ğŸŒ Transliterated '{text}' â†’ '{result}' (lang: {lang})")
    return result

def process_mixed_language_text(text, mode="transliterate"):
    """
    Xá»­ lÃ½ vÄƒn báº£n há»—n há»£p nhiá»u ngÃ´n ngá»¯.
    
    Args:
        text: VÄƒn báº£n Ä‘áº§u vÃ o
        mode: "transliterate" (phiÃªn Ã¢m), "remove" (xÃ³a), "keep" (giá»¯ nguyÃªn)
    
    Returns:
        VÄƒn báº£n Ä‘Ã£ xá»­ lÃ½
    """
    if not LANG_DETECT_AVAILABLE and mode == "transliterate":
        print("âš ï¸ Langdetect not available, keeping original text")
        return text
    
    words = text.split()
    processed_words = []
    
    for word in words:
        # Giá»¯ nguyÃªn sá»‘ vÃ  dáº¥u cÃ¢u
        if not any(c.isalpha() for c in word):
            processed_words.append(word)
            continue
        
        # TÃ¡ch dáº¥u cÃ¢u
        match = re.match(r'^([^\w]*)([\w]+)([^\w]*)$', word, re.UNICODE)
        if not match:
            processed_words.append(word)
            continue
        
        prefix, core_word, suffix = match.groups()
        
        # Kiá»ƒm tra cÃ³ pháº£i tiáº¿ng Viá»‡t khÃ´ng
        if is_vietnamese_word(core_word):
            processed_words.append(word)
        else:
            if mode == "transliterate":
                # Chuyá»ƒn sang phiÃªn Ã¢m
                transliterated = transliterate_to_vietnamese(core_word)
                processed_words.append(prefix + transliterated + suffix)
            elif mode == "remove":
                # XÃ³a tá»« ngoáº¡i ngá»¯
                print(f"   ğŸš« Removed foreign word: '{word}'")
                continue
            else:  # keep
                processed_words.append(word)
    
    return ' '.join(processed_words)

def split_text_into_sentences(text, pause_paragraph_duration=0.5, pause_dialogue_duration=0.25, 
                              foreign_word_mode="transliterate"):
    """
    TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u, tá»± Ä‘á»™ng xá»­ lÃ½ tá»« ngoáº¡i ngá»¯.
    
    Args:
        foreign_word_mode: "transliterate" (phiÃªn Ã¢m), "remove" (xÃ³a), "keep" (giá»¯ nguyÃªn)
    """
    # Xá»­ lÃ½ tá»« ngoáº¡i ngá»¯
    text = process_mixed_language_text(text, mode=foreign_word_mode)
    
    chunks = []
    paragraphs = text.split('\n\n')
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        lines = para.split('\n')
        combined_text = ' '.join(line.strip() for line in lines if line.strip())
        
        # PhÃ¡t hiá»‡n há»™i thoáº¡i
        open_quotes = combined_text.count('"') + combined_text.count('"')
        close_quotes = combined_text.count('"') + combined_text.count('"')
        is_dialogue = (open_quotes > 0 and open_quotes == close_quotes)
        pause_duration = pause_dialogue_duration if is_dialogue else pause_paragraph_duration
        
        # Loáº¡i bá» ngoáº·c kÃ©p
        clean_text = combined_text.replace('"', '').replace('"', '').replace('"', '').strip()
        
        # TÃ¡ch cÃ¢u
        sentences = re.split(r'([.!?]+)', clean_text)
        
        current_sentence = ""
        for i, part in enumerate(sentences):
            if i % 2 == 0:
                current_sentence += part
            else:
                current_sentence += part
                sentence_text = current_sentence.strip()
                
                if sentence_text and len(sentence_text.split()) >= 1:
                    chunks.append((sentence_text, pause_duration))
                    current_sentence = ""
                elif sentence_text:
                    current_sentence += " "
        
        if current_sentence.strip() and len(current_sentence.strip().split()) >= 3:
            chunks.append((current_sentence.strip(), pause_duration))
    
    # Gá»™p cÃ¢u ngáº¯n
    merged_chunks = []
    temp_sentence = ""
    temp_pause = pause_paragraph_duration
    
    for i, (sentence, pause) in enumerate(chunks):
        word_count = len(sentence.split())
        is_last = (i == len(chunks) - 1)
        
        if word_count >= 5:
            if temp_sentence:
                merged_chunks.append((temp_sentence + " " + sentence, pause))
                temp_sentence = ""
            else:
                merged_chunks.append((sentence, pause))
        else:
            if temp_sentence:
                temp_sentence += " " + sentence
            else:
                temp_sentence = sentence
                temp_pause = pause
            
            should_output = (len(temp_sentence.split()) >= 5) or (is_last and len(temp_sentence.split()) >= 1)
            
            if should_output:
                merged_chunks.append((temp_sentence, temp_pause))
                temp_sentence = ""
    
    if temp_sentence and len(temp_sentence.split()) >= 2:
        merged_chunks.append((temp_sentence, temp_pause))
    
    return merged_chunks

def create_silence(duration_seconds, sample_rate=24000):
    """Táº¡o Ä‘oáº¡n im láº·ng."""
    num_samples = int(duration_seconds * sample_rate)
    return np.zeros(num_samples, dtype=np.float32)

def apply_fade(audio, fade_samples, fade_type='out'):
    """Ãp dá»¥ng fade in/out."""
    if len(audio) < fade_samples:
        fade_samples = len(audio)
    
    fade_curve = np.linspace(0, 1, fade_samples) if fade_type == 'in' else np.linspace(1, 0, fade_samples)
    
    if fade_type == 'in':
        audio[:fade_samples] = audio[:fade_samples] * fade_curve
    else:
        audio[-fade_samples:] = audio[-fade_samples:] * fade_curve
    
    return audio

def post_process(text):
    """LÃ m sáº¡ch vÄƒn báº£n."""
    text = " " + text + " "
    text = text.replace(" . . ", " . ")
    text = text.replace(" .. ", " . ")
    text = text.replace('"', "")
    text = text.replace('"', "")
    text = text.replace('"', "")
    text = re.sub(r',+', ',', text)
    return " ".join(text.split())

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_level: str = "Medium", cross_fade_duration: float = 0.15,
              foreign_word_mode: str = "Transliterate", request: gr.Request = None):
    """TTS inference vá»›i xá»­ lÃ½ tá»± Ä‘á»™ng tá»« ngoáº¡i ngá»¯."""
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        # Cáº¥u hÃ¬nh pause
        pause_configs = {
            "Short": (0.3, 0.15),
            "Medium": (0.5, 0.25),
            "Long": (0.8, 0.4)
        }
        
        pause_paragraph, pause_dialogue = pause_configs.get(pause_level, (0.5, 0.25))
        
        # Chuyá»ƒn Ä‘á»•i mode
        mode_map = {
            "Transliterate": "transliterate",
            "Remove": "remove",
            "Keep": "keep"
        }
        mode = mode_map.get(foreign_word_mode, "transliterate")
        
        print(f"\nğŸ›ï¸ Config: Pause={pause_paragraph}s/{pause_dialogue}s, Foreign={mode}")
        print(f"ğŸ”€ Cross-fade: {cross_fade_duration}s")
        
        # TÃ¡ch vÄƒn báº£n
        chunks = split_text_into_sentences(gen_text, pause_paragraph, pause_dialogue, mode)
        
        print(f"\nğŸ“ Total chunks: {len(chunks)}")
        for idx, (sent, pause) in enumerate(chunks[:3], 1):
            print(f"   {idx}. [{pause}s] {sent[:60]}...")
        
        if not chunks:
            raise gr.Error("No valid sentences found in text.")
        
        # Preprocess reference audio
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # Táº¡o audio
        audio_segments = []
        sample_rate = 24000
        fade_samples = int(cross_fade_duration * sample_rate)
        
        for i, (sentence, pause_duration) in enumerate(chunks):
            print(f"\nğŸ”„ [{i+1}/{len(chunks)}] Processing: {sentence[:60]}...")
            
            try:
                normalized_text = post_process(TTSnorm(sentence)).lower()
            except Exception as e:
                print(f"   âŒ Error normalizing: {e}")
                continue
            
            if len(normalized_text.strip()) < 3:
                print(f"   â­ï¸ Skipped (too short): '{normalized_text}'")
                continue
            
            print(f"   ğŸ“ Normalized: {normalized_text[:80]}...")
            
            try:
                wave, sr, _ = infer_process(
                    ref_audio, 
                    ref_text.lower(), 
                    normalized_text, 
                    model, 
                    vocoder, 
                    speed=speed,
                    cross_fade_duration=cross_fade_duration
                )
                
                sample_rate = sr
                
                if i < len(chunks) - 1:
                    wave = apply_fade(wave.copy(), fade_samples, 'out')
                
                audio_segments.append(wave)
                print(f"   âœ… Generated {len(wave)/sr:.2f}s audio")
                
                if i < len(chunks) - 1:
                    silence = create_silence(pause_duration, sample_rate)
                    audio_segments.append(silence)
                    print(f"   â¸ï¸  Added {pause_duration}s silence")
                    
            except Exception as e:
                print(f"   âŒ Error processing: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if not audio_segments:
            raise gr.Error("No valid audio segments generated.")
            
        final_wave = np.concatenate(audio_segments)
        
        print(f"\nâœ… Final audio: {len(final_wave)/sample_rate:.2f}s")
        
        # Táº¡o spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            plt.specgram(final_wave, Fs=sample_rate, cmap='viridis')
            plt.xlabel('Time (s)')
            plt.ylabel('Frequency (Hz)')
            plt.title('Audio Spectrogram')
            plt.colorbar(format='%+2.0f dB')
            plt.tight_layout()
            plt.savefig(spectrogram_path)
            plt.close()

        return (sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error generating voice: {e}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(f"""
    # ğŸ¤ F5-TTS: Vietnamese TTS with Auto Foreign Word Handling
    
    ### âœ¨ Features:
    - **ğŸŒ Auto-detect foreign languages** (English, Chinese, Thai, Hindi, etc.)
    - **ğŸ”„ Auto-transliterate** to Vietnamese pronunciation
    - **â¸ï¸ Smart pauses** between sentences
    - **ğŸ”€ Smooth transitions** with cross-fade
    
    {"âœ… **Language detection enabled** (langdetect installed)" if LANG_DETECT_AVAILABLE else "âš ï¸ **Install langdetect for auto-detection**: `pip install langdetect unidecode`"}
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="ğŸ”Š Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="ğŸ“ Text to Generate", 
            placeholder="""Enter mixed language text...

Tiáº¿ng Viá»‡t:
Háº¯n ngá»“i trÃªn boong tÃ u. "Hello, how are you?" háº¯n há»i.

English + French:
The weather is nice. "Merci beaucoup!" he said.

ä¸­æ–‡ + à¹„à¸—à¸¢:
ä½ å¥½ä¸–ç•Œã€‚à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸šã€‚""", 
            lines=10
        )
    
    with gr.Row():
        speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="âš¡ Speed")
        pause_level = gr.Radio(
            choices=["Short", "Medium", "Long"],
            value="Medium",
            label="â¸ï¸ Pause Duration"
        )
    
    with gr.Row():
        cross_fade = gr.Slider(
            0.05, 0.3, value=0.15, step=0.05, 
            label="ğŸ”€ Cross-fade (s)"
        )
        foreign_word_mode = gr.Radio(
            choices=["Transliterate", "Remove", "Keep"],
            value="Transliterate",
            label="ğŸŒ Foreign Words",
            info="How to handle non-Vietnamese words"
        )
    
    btn_synthesize = gr.Button("ğŸ”¥ Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="ğŸ§ Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="ğŸ“Š Spectrogram")
    
    gr.Markdown("""
    ### ğŸŒ Foreign Word Modes:
    
    | Mode | Description | Example |
    |------|-------------|---------|
    | **Transliterate** âœ… | Convert to Vietnamese sound | "Hello" â†’ "hÃª lÃ´" |
    | **Remove** | Delete foreign words | "Hello world" â†’ "world" (if Vietnamese) |
    | **Keep** | Keep original (may fail) | "Hello" â†’ model error |
    
    ### ğŸ”„ Auto-transliteration Examples:
    
    ```
    English:  "Hello world"     â†’ "hÃª lÃ´ oa Ä‘Æ¡"
    French:   "Merci beaucoup"  â†’ "máº¹c xi bÃ´ cu"
    Chinese:  "ä½ å¥½" (nÇ hÇo)   â†’ "ni hao"
    Thai:     "à¸ªà¸§à¸±à¸ªà¸”à¸µ" (sawatdi)â†’ "sawatdi"
    Hindi:    "à¤¨à¤®à¤¸à¥à¤¤à¥‡" (namaste)â†’ "namaste"
    ```
    
    ### ğŸ’¡ How It Works:
    1. **Detect** language of each word (langdetect)
    2. **Transliterate** to Latin script (unidecode)
    3. **Adjust** pronunciation for Vietnamese TTS
    4. **Generate** natural speech
    
    ### ğŸ“¦ Installation (for full features):
    ```bash
    pip install langdetect unidecode
    ```
    
    ### âš ï¸ Limitations:
    - Transliteration is phonetic approximation, not perfect
    - Complex foreign phrases may sound unnatural
    - Best for simple foreign words/names
    """)

    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, pause_level, cross_fade, foreign_word_mode], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)
