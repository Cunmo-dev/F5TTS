import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re
import numpy as np

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def clean_text_before_processing(text):
    """L√†m s·∫°ch text: lo·∫°i b·ªè emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát."""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "\U0001F900-\U0001F9FF"
        "\U0001FA00-\U0001FA6F"
        "]+", 
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)
    return ' '.join(text.split()).strip()

def analyze_text_structure(text):
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c vƒÉn b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh v·ªã tr√≠ c·∫ßn ch√®n silence.
    
    Returns:
        list of dict: [
            {
                'text': 'c√¢u vƒÉn',
                'is_dialogue': True/False,
                'needs_pause_after': True/False,
                'char_start': 0,
                'char_end': 10
            },
            ...
        ]
    """
    text = clean_text_before_processing(text)
    paragraphs = text.split('\n\n')
    
    segments = []
    current_char_pos = 0
    
    for para_idx, para in enumerate(paragraphs):
        para = para.strip()
        if not para:
            continue
        
        # G·ªôp c√°c d√≤ng trong ƒëo·∫°n
        lines = para.split('\n')
        combined_text = ' '.join(line.strip() for line in lines if line.strip())
        
        # Ki·ªÉm tra h·ªôi tho·∫°i
        has_quotes = '"' in combined_text or '"' in combined_text or '"' in combined_text
        is_dialogue = has_quotes
        
        # T√¨m c√°c c√¢u trong ƒëo·∫°n n√†y
        # Pattern: t√°ch theo d·∫•u c√¢u nh∆∞ng GI·ªÆ NGUY√äN trong ngo·∫∑c k√©p
        sentences = []
        
        # Tr√≠ch xu·∫•t c√°c ƒëo·∫°n trong ngo·∫∑c k√©p
        quoted_pattern = r'(["""])([^"""]+)(["""])'
        quoted_ranges = []
        for match in re.finditer(quoted_pattern, combined_text):
            quoted_ranges.append((match.start(2), match.end(2)))
            # Th√™m to√†n b·ªô c√¢u trong ngo·∫∑c k√©p
            quoted_text = match.group(2).strip()
            sentences.append({
                'text': quoted_text,
                'is_dialogue': True,
                'start_in_para': match.start(2),
                'end_in_para': match.end(2)
            })
        
        # T√°ch c√°c c√¢u NGO√ÄI ngo·∫∑c k√©p
        # Thay th·∫ø ph·∫ßn trong ngo·∫∑c k√©p b·∫±ng placeholder
        temp_text = combined_text
        for start, end in sorted(quoted_ranges, reverse=True):
            temp_text = temp_text[:start] + '###QUOTED###' + temp_text[end:]
        
        # T√°ch theo d·∫•u c√¢u
        parts = re.split(r'([.!?]+)', temp_text)
        
        current_pos_in_para = 0
        for i in range(0, len(parts) - 1, 2):
            sentence_text = parts[i].strip()
            punctuation = parts[i + 1] if i + 1 < len(parts) else ''
            
            if sentence_text and sentence_text != '###QUOTED###':
                full_sentence = sentence_text + punctuation
                # T√¨m v·ªã tr√≠ th·ª±c trong combined_text
                actual_start = combined_text.find(full_sentence, current_pos_in_para)
                if actual_start != -1:
                    sentences.append({
                        'text': full_sentence,
                        'is_dialogue': False,
                        'start_in_para': actual_start,
                        'end_in_para': actual_start + len(full_sentence)
                    })
                    current_pos_in_para = actual_start + len(full_sentence)
        
        # S·∫Øp x·∫øp theo v·ªã tr√≠ xu·∫•t hi·ªán
        sentences.sort(key=lambda x: x['start_in_para'])
        
        # Th√™m v√†o segments v·ªõi v·ªã tr√≠ tuy·ªát ƒë·ªëi
        para_start_pos = text.find(combined_text)
        for sent_idx, sent in enumerate(sentences):
            is_last_in_para = (sent_idx == len(sentences) - 1)
            is_last_para = (para_idx == len(paragraphs) - 1)
            
            segments.append({
                'text': sent['text'],
                'is_dialogue': sent['is_dialogue'],
                'needs_pause_after': not (is_last_in_para and is_last_para),  # Kh√¥ng pause ·ªü c√¢u cu·ªëi
                'char_start': para_start_pos + sent['start_in_para'],
                'char_end': para_start_pos + sent['end_in_para']
            })
        
        current_char_pos += len(para) + 2  # +2 for \n\n
    
    return segments

def estimate_pause_positions(segments, total_audio_length, total_text_length):
    """
    ∆Ø·ªõc l∆∞·ª£ng v·ªã tr√≠ c·∫ßn ch√®n silence trong audio d·ª±a tr√™n v·ªã tr√≠ trong text.
    
    Returns:
        list of dict: [
            {
                'position_seconds': 1.5,
                'duration_seconds': 0.4,
                'is_dialogue': True/False
            },
            ...
        ]
    """
    pause_positions = []
    
    for seg in segments:
        if seg['needs_pause_after']:
            # ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian t∆∞∆°ng ·ª©ng trong audio
            # Gi·∫£ ƒë·ªãnh audio ph√¢n b·ªë ƒë·ªÅu theo text
            relative_position = seg['char_end'] / total_text_length
            audio_position = relative_position * total_audio_length
            
            pause_positions.append({
                'position_seconds': audio_position,
                'duration_seconds': 0.2 if seg['is_dialogue'] else 0.4,
                'is_dialogue': seg['is_dialogue']
            })
    
    return pause_positions

def insert_silences_into_audio(audio, sample_rate, pause_positions):
    """
    Ch√®n silence v√†o audio t·∫°i c√°c v·ªã tr√≠ ƒë√£ x√°c ƒë·ªãnh.
    
    Returns:
        numpy array: Audio m·ªõi v·ªõi silence ƒë√£ ch√®n
    """
    if not pause_positions:
        return audio
    
    # S·∫Øp x·∫øp theo v·ªã tr√≠
    pause_positions = sorted(pause_positions, key=lambda x: x['position_seconds'])
    
    segments = []
    last_pos = 0
    
    for pause in pause_positions:
        pos_samples = int(pause['position_seconds'] * sample_rate)
        
        # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° ƒë·ªô d√†i audio
        if pos_samples > len(audio):
            pos_samples = len(audio)
        
        # Th√™m ph·∫ßn audio tr∆∞·ªõc pause
        if pos_samples > last_pos:
            segments.append(audio[last_pos:pos_samples])
        
        # Th√™m silence
        silence_samples = int(pause['duration_seconds'] * sample_rate)
        silence = np.zeros(silence_samples, dtype=audio.dtype)
        segments.append(silence)
        
        print(f"   ‚è∏Ô∏è  Inserted {pause['duration_seconds']}s silence at {pause['position_seconds']:.2f}s ({'dialogue' if pause['is_dialogue'] else 'paragraph'})")
        
        last_pos = pos_samples
    
    # Th√™m ph·∫ßn audio c√≤n l·∫°i
    if last_pos < len(audio):
        segments.append(audio[last_pos:])
    
    return np.concatenate(segments)

def post_process(text):
    """L√†m s·∫°ch vƒÉn b·∫£n - lo·∫°i b·ªè t·∫•t c·∫£ k√Ω t·ª± ƒë·∫∑c bi·ªát."""
    # Lo·∫°i b·ªè ngo·∫∑c k√©p
    text = text.replace('"', '').replace('"', '').replace('"', '')
    
    # Lo·∫°i b·ªè t·∫•t c·∫£ d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = " ".join(text.split()).strip()
    
    return text

def safe_normalize(text):
    """Normalize vƒÉn b·∫£n an to√†n."""
    try:
        normalized = TTSnorm(text)
        if len(normalized.strip()) < 2:
            return text.lower()
        return normalized.lower()
    except Exception as e:
        print(f"   ‚ö†Ô∏è TTSnorm error: {e}, using original text")
        return text.lower()

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_paragraph: float = 0.4, pause_dialogue: float = 0.2, request: gr.Request = None):
    """
    TTS inference - Hybrid approach:
    1. X·ª≠ l√Ω to√†n b·ªô vƒÉn b·∫£n m·ªôt l·∫ßn (·ªïn ƒë·ªãnh)
    2. Ch√®n silence v√†o audio sau khi sinh (ch√≠nh x√°c)
    """
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        print(f"\n{'='*60}")
        print(f"üé§ Starting Hybrid TTS Generation")
        print(f"{'='*60}")
        print(f"üéõÔ∏è Pause config: Paragraph={pause_paragraph}s, Dialogue={pause_dialogue}s")
        
        # B∆∞·ªõc 1: Ph√¢n t√≠ch c·∫•u tr√∫c vƒÉn b·∫£n
        print(f"\nüìä Analyzing text structure...")
        segments = analyze_text_structure(gen_text)
        
        print(f"   Found {len(segments)} segments:")
        for idx, seg in enumerate(segments[:5], 1):
            marker = "üí¨" if seg['is_dialogue'] else "üìÑ"
            pause_marker = "‚è∏Ô∏è" if seg['needs_pause_after'] else "üîö"
            print(f"   {idx}. {marker}{pause_marker} [{seg['char_start']}-{seg['char_end']}] {seg['text'][:60]}...")
        
        # B∆∞·ªõc 2: Chu·∫©n b·ªã text cho TTS
        print(f"\nüìù Preparing text for TTS...")
        clean_text = clean_text_before_processing(gen_text)
        normalized_text = safe_normalize(clean_text)
        normalized_text = post_process(normalized_text)
        
        print(f"   Original: {len(gen_text)} chars")
        print(f"   Normalized: {len(normalized_text)} chars, {len(normalized_text.split())} words")
        print(f"   Preview: {normalized_text[:150]}...")
        
        # B∆∞·ªõc 3: Preprocess reference audio
        print(f"\nüîÑ Processing reference audio...")
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        print(f"   Reference text: {ref_text[:100]}...")
        
        # B∆∞·ªõc 4: Sinh audio cho TO√ÄN B·ªò vƒÉn b·∫£n m·ªôt l·∫ßn
        print(f"\nüéµ Generating audio for entire text...")
        wave, sr, _ = infer_process(
            ref_audio, 
            ref_text.lower(), 
            normalized_text, 
            model, 
            vocoder, 
            speed=speed
        )
        
        initial_duration = len(wave) / sr
        print(f"   ‚úÖ Generated {initial_duration:.2f}s audio")
        
        # B∆∞·ªõc 5: ∆Ø·ªõc l∆∞·ª£ng v·ªã tr√≠ pause trong audio
        print(f"\nüéØ Calculating pause positions...")
        pause_positions = estimate_pause_positions(
            segments, 
            initial_duration, 
            len(clean_text)
        )
        
        # C·∫≠p nh·∫≠t pause duration t·ª´ config
        for pause in pause_positions:
            if pause['is_dialogue']:
                pause['duration_seconds'] = pause_dialogue
            else:
                pause['duration_seconds'] = pause_paragraph
        
        print(f"   Found {len(pause_positions)} pause points")
        
        # B∆∞·ªõc 6: Ch√®n silence v√†o audio
        print(f"\n‚è∏Ô∏è  Inserting silences...")
        final_wave = insert_silences_into_audio(wave, sr, pause_positions)
        
        final_duration = len(final_wave) / sr
        added_silence = final_duration - initial_duration
        
        print(f"\n‚úÖ Audio processing complete!")
        print(f"   Initial duration: {initial_duration:.2f}s")
        print(f"   Added silence: {added_silence:.2f}s")
        print(f"   Final duration: {final_duration:.2f}s")
        print(f"{'='*60}\n")
        
        # T·∫°o spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            plt.specgram(final_wave, Fs=sr, cmap='viridis')
            plt.xlabel('Time (s)')
            plt.ylabel('Frequency (Hz)')
            plt.title('Audio Spectrogram')
            plt.colorbar(format='%+2.0f dB')
            plt.tight_layout()
            plt.savefig(spectrogram_path)
            plt.close()

        return (sr, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error generating voice: {str(e)}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech (Hybrid Approach)
    
    ### ‚ú® New Hybrid Method:
    - **Step 1**: Process entire text at once (stable, no skipping)
    - **Step 2**: Insert precise silences into generated audio
    - **Result**: Best of both worlds! üéâ
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text to Generate", 
            placeholder="""Enter Vietnamese text here...

Example with dialogue:
H·∫Øn ng·ªìi tr√™n t√†u. M·∫Øt nh√¨n ra bi·ªÉn xa.

"M·∫•y nƒÉm qua... em ƒë√£ s·ªëng th·∫ø n√†o?"

C√¥ g√°i im l·∫∑ng.
""", 
            lines=10
        )
    
    with gr.Row():
        speed = gr.Slider(
            minimum=0.3, 
            maximum=2.0, 
            value=1.0, 
            step=0.1, 
            label="‚ö° Speed"
        )
        pause_paragraph = gr.Slider(
            minimum=0.0,
            maximum=1.0,
            value=0.4,
            step=0.05,
            label="‚è∏Ô∏è Pause (Paragraph)"
        )
        pause_dialogue = gr.Slider(
            minimum=0.0,
            maximum=1.0,
            value=0.2,
            step=0.05,
            label="‚è∏Ô∏è Pause (Dialogue)"
        )
    
    btn_synthesize = gr.Button("üî• Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üéØ How It Works:
    
    1. **Text Analysis**: Detects dialogue (in quotes) vs narrative text
    2. **Single-Pass Generation**: Processes all text at once (no errors!)
    3. **Smart Pause Insertion**: Adds silences based on text structure
    4. **Precise Control**: Separate pause durations for dialogue vs paragraphs
    
    ### ‚úÖ Advantages:
    - ‚ú® **Stable**: No failed chunks or skipped sentences
    - üéØ **Precise**: Silences inserted at exact positions
    - üí¨ **Smart**: Automatically detects dialogue vs narrative
    - ‚ö° **Fast**: Single model inference
    
    ### üìù Tips:
    - Use **double line breaks** (`\\n\\n`) to separate paragraphs
    - Put dialogue in quotes: `"Hello!"`
    - Adjust pause sliders to taste
    """)

    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, pause_paragraph, pause_dialogue], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)
