def split_text_into_sentences(text, pause_paragraph_duration=0.8, pause_dialogue_duration=0.4):
    """
    T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u v·ªõi th√¥ng tin pause.
    
    Returns:
        list of tuples: [(sentence, pause_after_in_seconds, sentence_group_id), ...]
        - C√°c c√¢u c√πng group s·∫Ω ƒë∆∞·ª£c merge audio v·ªõi silence gi·ªØa ch√∫ng
    """
    chunks = []
    
    # T√°ch theo d√≤ng tr·ªëng
    paragraphs = text.split('\n\n')
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # X√°c ƒë·ªãnh lo·∫°i n·ªôi dung
        lines = para.split('\n')
        combined_text = ' '.join(line.strip() for line in lines if line.strip())
        
        open_quotes = combined_text.count('"') + combined_text.count('"')
        close_quotes = combined_text.count('"') + combined_text.count('"')
        is_dialogue = (open_quotes > 0 and open_quotes == close_quotes)
        pause_duration = pause_dialogue_duration if is_dialogue else pause_paragraph_duration
        
        clean_text = combined_text.replace('"', '').replace('"', '').replace('"', '').strip()
        
        # T√°ch c√¢u
        sentences = re.split(r'([.!?]+)', clean_text)
        
        current_sentence = ""
        for i, part in enumerate(sentences):
            if i % 2 == 0:
                current_sentence += part
            else:
                current_sentence += part
                sentence_text = current_sentence.strip()
                if sentence_text:
                    chunks.append((sentence_text, pause_duration, None))
                    current_sentence = ""
        
        if current_sentence.strip():
            chunks.append((current_sentence.strip(), pause_duration, None))
    
    # Gom nh√≥m c√°c c√¢u ng·∫Øn - ch√∫ng s·∫Ω ƒë∆∞·ª£c generate ri√™ng nh∆∞ng gh√©p v·ªõi silence
    processed_chunks = []
    temp_group = []  # [(sentence, pause), ...]
    group_id = 0
    temp_pause = pause_paragraph_duration
    
    for i, (sentence, pause, _) in enumerate(chunks):
        word_count = len(sentence.split())
        is_last = (i == len(chunks) - 1)
        
        if word_count >= 5:
            # C√¢u d√†i: xu·∫•t group tr∆∞·ªõc (n·∫øu c√≥), r·ªìi th√™m c√¢u n√†y
            if temp_group:
                # Xu·∫•t group c√°c c√¢u ng·∫Øn
                for s, p in temp_group:
                    processed_chunks.append((s, p, group_id))
                group_id += 1
                temp_group = []
            
            # Th√™m c√¢u d√†i (ƒë·ªôc l·∫≠p)
            processed_chunks.append((sentence, pause, group_id))
            group_id += 1
        else:
            # C√¢u ng·∫Øn: th√™m v√†o group
            temp_group.append((sentence, pause))
            temp_pause = pause
            
            # Ki·ªÉm tra xem c√≥ n√™n xu·∫•t group kh√¥ng
            total_words = sum(len(s.split()) for s in [s for s, _ in temp_group])
            should_output = total_words >= 5
            
            if should_output or is_last:
                # Xu·∫•t group
                for s, p in temp_group:
                    processed_chunks.append((s, p, group_id))
                group_id += 1
                temp_group = []
    
    # X·ª≠ l√Ω group c√≤n s√≥t (n·∫øu c√≥)
    if temp_group:
        for s, p in temp_group:
            processed_chunks.append((s, p, group_id))
    
    # Log th√¥ng tin
    print(f"\nüì¶ Grouped sentences:")
    current_group = None
    for sentence, pause, gid in processed_chunks[:10]:
        if gid != current_group:
            print(f"\n   Group {gid}:")
            current_group = gid
        print(f"      - [{len(sentence.split())}w, {pause}s] {sentence[:60]}...")
    
    return processed_chunks


@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_level: str = "Medium", request: gr.Request = None):
    """
    TTS v·ªõi pause th·ª±c s·ª± - Generate ri√™ng t·ª´ng c√¢u, gh√©p audio v·ªõi silence.
    """
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        # C·∫•u h√¨nh pause
        pause_configs = {
            "Short": {
                "paragraph": 0.3,
                "dialogue": 0.15,
                "within_group": 0.15  # Pause gi·ªØa c√°c c√¢u trong c√πng group
            },
            "Medium": {
                "paragraph": 0.5,
                "dialogue": 0.25,
                "within_group": 0.3
            },
            "Long": {
                "paragraph": 0.8,
                "dialogue": 0.4,
                "within_group": 0.5
            }
        }
        
        config = pause_configs.get(pause_level, pause_configs["Medium"])
        
        print(f"\nüéõÔ∏è Pause config: {config}")
        
        # T√°ch v√† gom nh√≥m c√¢u
        chunks = split_text_into_sentences(
            gen_text, 
            config["paragraph"], 
            config["dialogue"]
        )
        
        if not chunks:
            raise gr.Error("No valid sentences found.")
        
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        audio_segments = []
        sample_rate = 24000
        
        # X·ª≠ l√Ω t·ª´ng c√¢u
        current_group = None
        for i, (sentence, pause_duration, group_id) in enumerate(chunks):
            is_new_group = (group_id != current_group)
            is_last_in_group = (i == len(chunks) - 1 or chunks[i+1][2] != group_id)
            
            print(f"\nüîÑ [{i+1}/{len(chunks)}] Group {group_id}: {sentence[:60]}...")
            
            # Chu·∫©n h√≥a
            normalized_text = post_process(safe_normalize(sentence))
            normalized_text = validate_text_for_tts(normalized_text)
            
            word_count = len(normalized_text.strip().split())
            if word_count < 2:
                print(f"   ‚è≠Ô∏è Skipped: too short ({word_count} words)")
                continue
            
            print(f"   üìù Normalized ({word_count} words): {normalized_text[:60]}...")
            
            # Generate audio v·ªõi retry
            max_retries = 2
            retry_count = 0
            success = False
            
            while retry_count <= max_retries and not success:
                try:
                    if retry_count > 0:
                        print(f"   üîÅ Retry {retry_count}/{max_retries}...")
                    
                    wave, sr, _ = infer_process(
                        ref_audio, 
                        ref_text.lower(), 
                        normalized_text, 
                        model, 
                        vocoder, 
                        speed=speed
                    )
                    
                    sample_rate = sr
                    audio_segments.append(wave)
                    print(f"   ‚úÖ Generated {len(wave)/sr:.2f}s audio")
                    success = True
                    
                    # Th√™m silence d·ª±a tr√™n v·ªã tr√≠
                    if not is_last_in_group:
                        # Gi·ªØa c√°c c√¢u trong c√πng group: pause ng·∫Øn
                        silence = create_silence(config["within_group"], sample_rate)
                        audio_segments.append(silence)
                        print(f"   ‚è∏Ô∏è  Within-group pause: {config['within_group']}s")
                    elif i < len(chunks) - 1:
                        # Gi·ªØa c√°c group: pause d√†i
                        silence = create_silence(pause_duration, sample_rate)
                        audio_segments.append(silence)
                        print(f"   ‚è∏Ô∏è‚è∏Ô∏è  Between-group pause: {pause_duration}s")
                        
                except Exception as e:
                    retry_count += 1
                    print(f"   ‚ö†Ô∏è Attempt {retry_count} failed: {str(e)[:100]}")
                    
                    if retry_count > max_retries:
                        print(f"   ‚ùå Max retries reached")
                        # Fallback: th·ª≠ v·ªõi 3 t·ª´ ƒë·∫ßu
                        if len(normalized_text.split()) > 3:
                            print(f"   üîß Trying first 3 words...")
                            simplified_text = ' '.join(normalized_text.split()[:3])
                            try:
                                wave, sr, _ = infer_process(
                                    ref_audio, 
                                    ref_text.lower(), 
                                    simplified_text, 
                                    model, 
                                    vocoder, 
                                    speed=speed
                                )
                                sample_rate = sr
                                audio_segments.append(wave)
                                print(f"   ‚úÖ Simplified success")
                                success = True
                            except:
                                print(f"   ‚ùå Simplified also failed")
                        break
                    
                    import time
                    time.sleep(0.5)
            
            current_group = group_id
        
        if not audio_segments:
            raise gr.Error("No audio generated.")
            
        final_wave = np.concatenate(audio_segments)
        
        # T√≠nh s·ªë group
        num_groups = len(set(gid for _, _, gid in chunks))
        print(f"\n‚úÖ Final: {len(final_wave)/sample_rate:.2f}s from {len(chunks)} sentences in {num_groups} groups")
        
        # Spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(12, 4))
            plt.specgram(final_wave, Fs=sample_rate, cmap='viridis')
            plt.xlabel('Time (s)')
            plt.ylabel('Frequency (Hz)')
            plt.title('Audio Spectrogram')
            plt.colorbar(format='%+2.0f dB')
            plt.tight_layout()
            plt.savefig(spectrogram_path)
            plt.close()

        return (sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error: {str(e)}")
