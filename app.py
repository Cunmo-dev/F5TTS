
import spaces
import os
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import numpy as np
import re

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Set matplotlib backend
os.environ["MPLBACKEND"] = "Agg"

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def post_process(text):
    text = " " + text + " "
    text = text.replace(" . . ", " . ")
    text = " " + text + " "
    text = text.replace(" .. ", " . ")
    text = " " + text + " "
    text = text.replace(" , , ", " , ")
    text = " " + text + " "
    text = text.replace(" ,, ", " , ")
    text = " " + text + " "
    text = text.replace('"', "")
    return " ".join(text.split())

def split_sentences(text, max_words=50):
    """
    T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u v·ªõi x·ª≠ l√Ω ƒë·∫∑c bi·ªát cho h·ªôi tho·∫°i.
    Gi·ªõi h·∫°n m·ªói c√¢u kh√¥ng qu√° max_words t·ª´ ƒë·ªÉ tr√°nh qu√° t·∫£i model.
    """
    sentences = []
    
    # T√°ch theo xu·ªëng d√≤ng tr∆∞·ªõc
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # N·∫øu l√† h·ªôi tho·∫°i (b·∫Øt ƒë·∫ßu b·∫±ng d·∫•u ngo·∫∑c k√©p)
        if line.startswith('"') or line.startswith('"') or line.startswith('"'):
            # Chia nh·ªè h·ªôi tho·∫°i d√†i n·∫øu c·∫ßn
            if len(line.split()) > max_words:
                chunks = split_by_length(line, max_words)
                sentences.extend(chunks)
            else:
                sentences.append(line)
        else:
            # T√°ch theo d·∫•u ch·∫•m, ch·∫•m than, ch·∫•m h·ªèi tr∆∞·ªõc
            parts = re.split(r'(?<=[.!?])\s+', line)
            
            for part in parts:
                if not part.strip():
                    continue
                
                # N·∫øu c√¢u qu√° d√†i, t√°ch th√™m theo d·∫•u ph·∫©y
                if len(part.split()) > max_words:
                    sub_parts = re.split(r'(?<=[,;])\s+', part)
                    for sub in sub_parts:
                        if sub.strip() and len(sub.strip()) > 5:
                            # N·∫øu v·∫´n c√≤n d√†i, chia theo s·ªë t·ª´
                            if len(sub.split()) > max_words:
                                chunks = split_by_length(sub, max_words)
                                sentences.extend(chunks)
                            else:
                                sentences.append(sub.strip())
                else:
                    sentences.append(part.strip())
    
    # L·ªçc b·ªè c√°c c√¢u qu√° ng·∫Øn (< 5 k√Ω t·ª±)
    sentences = [s for s in sentences if len(s.strip()) > 5]
    
    return sentences

def split_by_length(text, max_words):
    """
    Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè theo s·ªë t·ª´ t·ªëi ƒëa.
    """
    words = text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_words:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def add_silence(audio_array, sample_rate, duration_ms=500):
    """
    Th√™m kho·∫£ng l·∫∑ng v√†o cu·ªëi audio array.
    """
    silence_samples = int(sample_rate * duration_ms / 1000)
    silence = np.zeros(silence_samples, dtype=audio_array.dtype)
    return np.concatenate([audio_array, silence])

def is_dialogue(text):
    """
    Ki·ªÉm tra xem c√¢u c√≥ ph·∫£i l√† h·ªôi tho·∫°i kh√¥ng.
    """
    text = text.strip()
    return text.startswith('"') or text.startswith('"') or text.startswith('"')

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              pause_paragraph: int = 800, pause_dialogue: int = 400, 
              request: gr.Request = None):
    """
    Args:
        pause_paragraph: kho·∫£ng l·∫∑ng sau ƒëo·∫°n vƒÉn t·∫£ (ms)
        pause_dialogue: kho·∫£ng l·∫∑ng sau c√¢u h·ªôi tho·∫°i (ms)
    """
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    
    try:
        # Preprocess reference audio
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u (gi·ªõi h·∫°n 50 t·ª´/c√¢u ƒë·ªÉ tr√°nh qu√° t·∫£i)
        sentences = split_sentences(gen_text, max_words=50)
        
        print(f"\n=== DETECTED {len(sentences)} SENTENCES ===")
        for i, sent in enumerate(sentences):
            print(f"Sentence {i+1}: {sent[:80]}...")
        print("=" * 50)
        
        if len(sentences) == 0:
            raise gr.Error("No valid sentences found in the text.")
        
        # Kh·ªüi t·∫°o danh s√°ch ƒë·ªÉ l∆∞u audio
        audio_segments = []
        sample_rate = None
        all_spectrograms = []
        
        # X·ª≠ l√Ω t·ª´ng c√¢u
        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue
            
            try:
                # Chu·∫©n h√≥a v√† x·ª≠ l√Ω c√¢u
                processed_sentence = post_process(TTSnorm(sentence)).lower()
                
                print(f"Processing {i+1}/{len(sentences)}: {processed_sentence[:60]}...")
                
                # T·∫°o audio cho c√¢u
                wave, sr, spectrogram = infer_process(
                    ref_audio, 
                    ref_text.lower(), 
                    processed_sentence, 
                    model, 
                    vocoder, 
                    speed=speed
                )
                
                if sample_rate is None:
                    sample_rate = sr
                
                # Th√™m kho·∫£ng l·∫∑ng ph√π h·ª£p
                if i < len(sentences) - 1:
                    # H·ªôi tho·∫°i: pause ng·∫Øn h∆°n
                    # ƒêo·∫°n vƒÉn t·∫£: pause d√†i h∆°n
                    pause = pause_dialogue if is_dialogue(sentence) else pause_paragraph
                    wave = add_silence(wave, sr, pause)
                
                audio_segments.append(wave)
                all_spectrograms.append(spectrogram)
                
            except Exception as e:
                print(f"Warning: Failed to process sentence {i+1}: {e}")
                # Ti·∫øp t·ª•c v·ªõi c√¢u ti·∫øp theo thay v√¨ d·ª´ng h·∫≥n
                continue
        
        if len(audio_segments) == 0:
            raise gr.Error("Failed to generate any audio segments.")
        
        # Gh√©p t·∫•t c·∫£ audio l·∫°i
        final_wave = np.concatenate(audio_segments)
        
        # T·∫°o spectrogram t·ªïng h·ª£p
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            save_spectrogram(all_spectrograms[0], spectrogram_path)

        return (sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        raise gr.Error(f"Error generating voice: {e}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech Synthesis
    # The model was trained with approximately 1000 hours of data on a RTX 3090 GPU
    Enter text and upload a sample voice to generate natural speech with intelligent pausing.
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text", 
            placeholder="Enter the text to generate voice (supports paragraphs and dialogue)...", 
            lines=8
        )
    
    with gr.Row():
        speed = gr.Slider(0.3, 2.0, value=1.0, step=0.1, label="‚ö° Speed")
        pause_paragraph = gr.Slider(
            200, 2000, value=800, step=100, 
            label="‚è∏Ô∏è Pause After Paragraph (ms)"
        )
        pause_dialogue = gr.Slider(
            100, 1500, value=400, step=50, 
            label="üí¨ Pause After Dialogue (ms)"
        )
    
    btn_synthesize = gr.Button("üî• Generate Voice")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üí° Tips:
    - **Paragraph Pause**: Longer silence after descriptive text (default 800ms)
    - **Dialogue Pause**: Shorter silence between dialogue lines (default 400ms)
    - System automatically detects dialogue (text in quotes) vs narration
    - For natural conversation flow, use 300-500ms for dialogue
    - For dramatic reading, increase paragraph pause to 1000-1500ms
    - **NEW**: Now splits sentences by commas (,) and semicolons (;) in addition to periods (.)
    """)
    
    model_limitations = gr.Textbox(
        value="""1. This model may not perform well with numerical characters, dates, special characters, etc. => A text normalization module is needed.
2. The rhythm of some generated audios may be inconsistent or choppy => It is recommended to select clearly pronounced sample audios with minimal pauses for better synthesis quality.
3. Default, reference audio text uses the pho-whisper-medium model, which may not always accurately recognize Vietnamese, resulting in poor voice synthesis quality.
4. Inference with overly long paragraphs may produce poor results.""", 
        label="‚ùó Model Limitations",
        lines=4,
        interactive=False
    )

    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, pause_paragraph, pause_dialogue], 
        outputs=[output_audio, output_spectrogram]
    )

# Run Gradio with share=True
demo.queue().launch(share=True)
