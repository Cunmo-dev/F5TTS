import os
os.environ["MPLBACKEND"] = "Agg"

import spaces
from huggingface_hub import login
import gradio as gr
from cached_path import cached_path
import tempfile
from vinorm import TTSnorm
import re

from f5_tts.model import DiT
from f5_tts.infer.utils_infer import (
    preprocess_ref_audio_text,
    load_vocoder,
    load_model,
    infer_process,
    save_spectrogram,
)

# Retrieve token from secrets
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Log in to Hugging Face
if hf_token:
    login(token=hf_token)

def is_repetitive_text(text):
    """
    Ki·ªÉm tra xem c√¢u c√≥ ph·∫£i l√† vƒÉn b·∫£n l·∫∑p l·∫°i kh√¥ng.
    V√≠ d·ª•: "H√° h√° h√°", "hu hu hu", "ha ha ha..."
    """
    clean_text = re.sub(r'[.!?,;:]', '', text).lower().strip()
    words = clean_text.split()
    
    if len(words) < 2:
        return False
    
    unique_words = set(words)
    if len(unique_words) == 1:
        return True
    
    # Lo·∫°i b·ªè d·∫•u thanh ƒë·ªÉ so s√°nh
    normalized_words = [''.join(c for c in word if c.isalpha()) for word in words]
    unique_normalized = set(normalized_words)
    
    if len(unique_normalized) == 1 and len(normalized_words) >= 2:
        return True
    
    if len(unique_normalized) <= max(2, len(words) * 0.3):
        return True
    
    return False

def normalize_sentence_ending(sentence):
    """
    Chu·∫©n h√≥a k√Ω t·ª± k·∫øt th√∫c c√¢u:
    - N·∫øu kh√¥ng c√≥ d·∫•u ch·∫•m c√¢u ‚Üí th√™m d·∫•u ch·∫•m
    - N·∫øu c√≥ d·∫•u ch·∫•m + k√Ω t·ª± ƒë·∫∑c bi·ªát ‚Üí x√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát
    """
    sentence = sentence.strip()
    
    # Danh s√°ch d·∫•u c√¢u h·ª£p l·ªá
    valid_punctuation = '.!?'
    
    # Ki·ªÉm tra k√Ω t·ª± cu·ªëi
    if not sentence:
        return sentence + "."
    
    last_char = sentence[-1]
    
    # N·∫øu ƒë√£ c√≥ d·∫•u c√¢u h·ª£p l·ªá
    if last_char in valid_punctuation:
        # X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát sau d·∫•u ch·∫•m (n·∫øu c√≥)
        while len(sentence) > 1 and sentence[-1] not in valid_punctuation:
            sentence = sentence[:-1]
        return sentence
    
    # Ki·ªÉm tra c√≥ d·∫•u c√¢u ·ªü v·ªã tr√≠ g·∫ßn cu·ªëi kh√¥ng
    for i in range(len(sentence) - 1, max(0, len(sentence) - 5), -1):
        if sentence[i] in valid_punctuation:
            # C√≥ d·∫•u c√¢u nh∆∞ng c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát ph√≠a sau ‚Üí c·∫Øt b·ªè
            return sentence[:i+1]
    
    # Kh√¥ng c√≥ d·∫•u c√¢u ‚Üí th√™m d·∫•u ch·∫•m
    return sentence + "."

def smart_text_preprocessing(text, silence_duration=0.4):
    """
    X·ª≠ l√Ω vƒÉn b·∫£n th√¥ng minh:
    - Ph√°t hi·ªán v√† x·ª≠ l√Ω c√¢u l·∫∑p l·∫°i (h√° h√° h√°)
    - G·ªôp c√¢u ng·∫Øn < 3 t·ª´ b·∫±ng d·∫•u ch·∫•m
    - Chu·∫©n h√≥a k√Ω t·ª± k·∫øt th√∫c c√¢u
    - Th√™m d·∫•u ch·∫•m l·∫∑p ƒë·ªÉ t·∫°o pause t·ª± nhi√™n (d·ª±a v√†o silence_duration)
    
    Returns:
        str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω, s·∫µn s√†ng ƒë·ªçc m·ªôt l·∫ßn
    """
    print("\nüìù Starting smart text preprocessing...")
    print(f"   Silence duration: {silence_duration}s (will add extra periods)")
    
    # T√≠nh s·ªë d·∫•u ch·∫•m c·∫ßn th√™m d·ª±a v√†o silence duration
    # 0.1-0.3s: 1 d·∫•u ch·∫•m
    # 0.4-0.6s: 2 d·∫•u ch·∫•m
    # 0.7-1.0s: 3 d·∫•u ch·∫•m
    if silence_duration <= 0.3:
        pause_marker = "."
        para_pause_marker = ". "
    elif silence_duration <= 0.6:
        pause_marker = ". "
        para_pause_marker = ". . "
    else:
        pause_marker = ". . "
        para_pause_marker = ". . . "
    
    print(f"   Using pause marker: '{pause_marker}' between sentences")
    print(f"   Using para marker: '{para_pause_marker}' between paragraphs")
    
    # T√°ch theo ƒëo·∫°n vƒÉn
    paragraphs = text.split('\n\n')
    processed_paragraphs = []
    
    for para_idx, para in enumerate(paragraphs):
        para = para.strip()
        if not para:
            continue
        
        print(f"\nüìÑ Processing paragraph {para_idx + 1}:")
        
        # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p ƒë·ªÉ x·ª≠ l√Ω
        clean_para = para.replace('"', '').replace('"', '').replace('"', '').strip()
        
        # T√°ch th√†nh c√°c c√¢u
        sentences = re.split(r'([.!?]+)', clean_para)
        
        processed_sentences = []
        temp_accumulator = []  # T√≠ch l≈©y c√¢u ng·∫Øn/l·∫∑p
        
        for i in range(0, len(sentences), 2):
            if i >= len(sentences):
                break
                
            sentence_text = sentences[i].strip()
            punctuation = sentences[i + 1] if i + 1 < len(sentences) else '.'
            
            if not sentence_text:
                continue
            
            full_sentence = sentence_text + punctuation
            word_count = len(sentence_text.split())
            is_repetitive = is_repetitive_text(sentence_text)
            
            print(f"   - '{sentence_text[:50]}...' ({word_count} words, repetitive: {is_repetitive})")
            
            # Ki·ªÉm tra xem c√≥ c·∫ßn g·ªôp kh√¥ng
            should_merge = (word_count < 3) or is_repetitive
            
            if should_merge:
                print(f"     ‚Üí Will merge (too short or repetitive)")
                temp_accumulator.append(sentence_text)
            else:
                # C√¢u ƒë·ªß d√†i
                if temp_accumulator:
                    # G·ªôp c√°c c√¢u t√≠ch l≈©y + c√¢u hi·ªán t·∫°i
                    merged = ". ".join(temp_accumulator + [sentence_text])
                    merged = normalize_sentence_ending(merged)
                    processed_sentences.append(merged)
                    print(f"     ‚Üí Merged with accumulated: '{merged[:60]}...'")
                    temp_accumulator = []
                else:
                    # C√¢u ƒë·ªôc l·∫≠p
                    normalized = normalize_sentence_ending(sentence_text + punctuation)
                    processed_sentences.append(normalized)
                    print(f"     ‚Üí Kept as is: '{normalized[:60]}...'")
        
        # X·ª≠ l√Ω c√¢u c√≤n s√≥t
        if temp_accumulator:
            if processed_sentences:
                # G·ªôp v√†o c√¢u tr∆∞·ªõc
                last_sentence = processed_sentences[-1].rstrip('.!?')
                merged = last_sentence + ". " + ". ".join(temp_accumulator)
                merged = normalize_sentence_ending(merged)
                processed_sentences[-1] = merged
                print(f"   üîó Merged remaining to last sentence")
            else:
                # Ch·ªâ c√≥ c√¢u ng·∫Øn ‚Üí gi·ªØ nguy√™n, KH√îNG th√™m g√¨ c·∫£
                merged = ". ".join(temp_accumulator)
                merged = normalize_sentence_ending(merged)
                processed_sentences.append(merged)
                print(f"   ‚ö†Ô∏è Only short sentences (kept as is): '{merged}'")
        
        # Gh√©p c√°c c√¢u trong ƒëo·∫°n v·ªõi pause marker (d·∫•u ch·∫•m)
        processed_para = pause_marker.join(processed_sentences)
        processed_paragraphs.append(processed_para)
        print(f"   ‚úÖ Paragraph result: '{processed_para[:80]}...'")
    
    # Gh√©p t·∫•t c·∫£ ƒëo·∫°n vƒÉn l·∫°i v·ªõi pause d√†i h∆°n
    final_text = para_pause_marker.join(processed_paragraphs)
    
    print(f"\n‚úÖ Preprocessing complete!")
    print(f"   Original length: {len(text)} chars")
    print(f"   Processed length: {len(final_text)} chars")
    print(f"   Preview: '{final_text[:100]}...'")
    
    return final_text

def post_process(text):
    """L√†m s·∫°ch vƒÉn b·∫£n."""
    text = " " + text + " "
    text = text.replace(" . . ", " . ")
    text = text.replace(" .. ", " . ")
    text = text.replace('"', "")
    text = text.replace('"', "")
    text = text.replace('"', "")
    text = re.sub(r',+', ',', text)
    text = re.sub(r'\.+', '.', text)  # Lo·∫°i b·ªè d·∫•u ch·∫•m tr√πng
    return " ".join(text.split())

def safe_normalize(text):
    """Normalize vƒÉn b·∫£n an to√†n."""
    try:
        normalized = TTSnorm(text)
        if len(normalized.strip()) < 2:
            return text.lower()
        return normalized.lower()
    except Exception as e:
        print(f"   ‚ö†Ô∏è TTSnorm error: {e}, using original text")
        return text.lower()

# Load models
vocoder = load_vocoder()
model = load_model(
    DiT,
    dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4),
    ckpt_path=str(cached_path("hf://thanhcong190693/F5TTSVN/model_last.pt")),
    vocab_file=str(cached_path("hf://thanhcong190693/F5TTSVN/config.json")),
)

@spaces.GPU
def infer_tts(ref_audio_orig: str, gen_text: str, speed: float = 1.0, 
              silence_duration: float = 0.4, use_smart_processing: bool = True, 
              request: gr.Request = None):
    """
    TTS inference v·ªõi x·ª≠ l√Ω th√¥ng minh nh∆∞ng v·∫´n ƒë·ªçc to√†n b·ªô m·ªôt l·∫ßn.
    """
    if not ref_audio_orig:
        raise gr.Error("Please upload a sample audio file.")
    if not gen_text.strip():
        raise gr.Error("Please enter the text content to generate voice.")
    if len(gen_text.split()) > 1000:
        raise gr.Error("Please enter text content with less than 1000 words.")
    
    try:
        # B∆∞·ªõc 1: Smart preprocessing (n·∫øu ƒë∆∞·ª£c b·∫≠t)
        if use_smart_processing:
            processed_text = smart_text_preprocessing(gen_text, silence_duration)
        else:
            processed_text = gen_text
            print("\nüìù Smart processing disabled, using original text")
        
        # B∆∞·ªõc 2: Normalize v√† clean
        print("\nüîÑ Normalizing text...")
        normalized_text = safe_normalize(processed_text)
        final_text = post_process(normalized_text)
        
        word_count = len(final_text.split())
        print(f"‚úÖ Final text ready ({word_count} words): '{final_text[:100]}...'")
        
        # B∆∞·ªõc 3: Preprocess reference audio
        print("\nüé§ Processing reference audio...")
        ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, "")
        
        # B∆∞·ªõc 4: Generate audio (m·ªôt l·∫ßn duy nh·∫•t)
        print("\nüéµ Generating audio (single pass)...")
        final_wave, final_sample_rate, spectrogram = infer_process(
            ref_audio, 
            ref_text.lower(), 
            final_text, 
            model, 
            vocoder, 
            speed=speed
        )
        
        duration = len(final_wave) / final_sample_rate
        print(f"‚úÖ Audio generated successfully!")
        print(f"   Duration: {duration:.2f}s")
        print(f"   Sample rate: {final_sample_rate}Hz")
        
        # B∆∞·ªõc 5: Save spectrogram
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_spectrogram:
            spectrogram_path = tmp_spectrogram.name
            save_spectrogram(spectrogram, spectrogram_path)

        return (final_sample_rate, final_wave), spectrogram_path
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise gr.Error(f"Error generating voice: {str(e)}")

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üé§ F5-TTS: Vietnamese Text-to-Speech (Hybrid Version)
    ### Model trained with ~1000 hours of data on RTX 3090 GPU
    
    **‚ú® Smart Single-Pass Processing:**
    - Detects and merges repetitive text ("H√° h√° h√°" ‚Üí merged with period)
    - Combines short sentences (< 3 words) automatically
    - Reads entire text in ONE go - no chunking errors!
    """)
    
    with gr.Row():
        ref_audio = gr.Audio(label="üîä Sample Voice", type="filepath")
        gen_text = gr.Textbox(
            label="üìù Text to Generate", 
            placeholder="""Enter text with paragraphs and dialogue...

Example:
H·∫Øn l√∫c n√†y ƒëang ng·ªìi tr√™n boong t√†u. M·∫Øt nh√¨n ra bi·ªÉn xa.

"H√° h√° h√°!"
"Toa l·∫ßn n√†y tr·ªü v·ªÅ nh√† ch∆°i ƒë∆∞·ª£c bao l√¢u?"

Ng∆∞·ªùi h·ªèi l√† m·ªôt ng∆∞·ªùi b·∫°n t√¨nh c·ªù g·∫∑p.""", 
            lines=10
        )
    
    with gr.Row():
        speed = gr.Slider(
            minimum=0.3, 
            maximum=2.0, 
            value=1.0, 
            step=0.1, 
            label="‚ö° Speed"
        )
        silence_duration = gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.4,
            step=0.1,
            label="‚è∏Ô∏è Silence Duration (seconds)",
            info="Control pause length between sentences"
        )
    
    use_smart_processing = gr.Checkbox(
        value=True,
        label="üß† Enable Smart Text Processing",
        info="Merge repetitive/short sentences before TTS"
    )
    
    btn_synthesize = gr.Button("üî• Generate Voice", variant="primary", size="lg")
    
    with gr.Row():
        output_audio = gr.Audio(label="üéß Generated Audio", type="numpy")
        output_spectrogram = gr.Image(label="üìä Spectrogram")
    
    gr.Markdown("""
    ### üí° How It Works:
    
    | Step | Description |
    |------|-------------|
    | 1Ô∏è‚É£ **Smart Preprocessing** | Merges short/repetitive sentences with periods |
    | 2Ô∏è‚É£ **Text Normalization** | Converts numbers, special chars to readable text |
    | 3Ô∏è‚É£ **Post Processing** | Cleans punctuation, whitespace |
    | 4Ô∏è‚É£ **Single-Pass TTS** | Reads entire text at once (no chunking!) |
    | 5Ô∏è‚É£ **Natural Pauses** | Model creates pauses at periods automatically |
    
    ### üéØ Smart Processing Examples:
    
    **Before:**
    ```
    "H√° h√° h√°!"
    "·ªí!"
    "Toa l·∫ßn n√†y tr·ªü v·ªÅ?"
    ```
    
    **After (with silence=0.4s):**
    ```
    "H√° h√° h√°. ·ªí. Toa l·∫ßn n√†y tr·ªü v·ªÅ?"
    ```
    ‚Üí Model reads as ONE audio with natural pauses at periods
    
    **How Silence Duration Works:**
    - **0.1-0.3s**: Single period between sentences (`.`)
    - **0.4-0.6s**: Double period (`. `) for longer pause
    - **0.7-1.0s**: Triple period (`. . `) for dramatic pause
    - **Between paragraphs**: Automatically uses longer pause
    
    ### ‚úÖ Advantages:
    - ‚úîÔ∏è No chunking errors
    - ‚úîÔ∏è Natural flow and rhythm
    - ‚úîÔ∏è Handles repetitive text (h√° h√°, h·ªÅ h·ªÅ, etc.)
    - ‚úîÔ∏è Merges ultra-short sentences
    - ‚úîÔ∏è Single GPU pass = faster + more stable
    - ‚úîÔ∏è Uses periods (not special markers) for pause control
    
    ### üìñ Usage Tips:
    - Separate paragraphs with double line breaks (`\\n\\n`)
    - Short sentences (< 3 words) will be merged automatically
    - Repetitive text like "H√° h√° h√°" gets merged intelligently
    - Disable smart processing if you want raw text only
    - Adjust silence slider to control pause length
    """)
    
    with gr.Accordion("‚ùó Model Limitations", open=False):
        gr.Markdown("""
        1. **Numbers & Special Characters**: May not pronounce dates/phone numbers correctly
        2. **Audio Quality**: Use clear reference audio without background noise
        3. **Reference Text**: Auto-transcribed with Whisper (may have errors)
        4. **Text Length**: Keep under 1000 words for best results
        5. **Foreign Words**: Pronounced phonetically in Vietnamese
        6. **Processing**: Single-pass means any error affects entire generation
        """)

    # Connect button
    btn_synthesize.click(
        infer_tts, 
        inputs=[ref_audio, gen_text, speed, silence_duration, use_smart_processing], 
        outputs=[output_audio, output_spectrogram]
    )

demo.queue().launch(share=True)
